{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quantum Machinery with gdb1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/torlarse/Programs/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb off\n",
    "__author__ = \"Joseph Gomes and Bharath Ramsundar\"\n",
    "__copyright__ = \"Copyright 2016, Stanford University\"\n",
    "__license__ = \"LGPL\"\n",
    "\n",
    "import os\n",
    "import unittest\n",
    "\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "import numpy.random\n",
    "from deepchem.utils.evaluate import Evaluator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = dc.feat.CoulombMatrixEig(23, remove_hydrogens=False)\n",
    "tasks = [\"atomization_energy\"]\n",
    "dataset_file = \"../../datasets/gdb1k.sdf\"\n",
    "smiles_field = \"smiles\"\n",
    "mol_field = \"mol\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load featurized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "Reading structures from ../../datasets/gdb1k.sdf.\n",
      "Currently featurizing feature_type: CoulombMatrixEig\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.494 s\n",
      "TIMING: dataset construction took 0.690 s\n",
      "Loading dataset from disk.\n",
      "type of diskData <class 'deepchem.data.datasets.DiskDataset'>\n"
     ]
    }
   ],
   "source": [
    "loader = dc.data.SDFLoader(\n",
    "      tasks=[\"atomization_energy\"], smiles_field=\"smiles\",\n",
    "      featurizer=featurizer,\n",
    "      mol_field=\"mol\")\n",
    "dataset = loader.featurize(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Train, Validation, and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 0.011 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.005 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.005 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "random_splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = random_splitter.train_valid_test_split(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 0.012 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.010 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.003 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.003 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.003 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.003 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "transformers = [\n",
    "    dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset),\n",
    "    dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]\n",
    "\n",
    "for dataset in [train_dataset, valid_dataset, test_dataset]:\n",
    "  for transformer in transformers:\n",
    "      dataset = transformer.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Fit Random Forest with hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1/8\n",
      "hyperparameters: {'n_estimators': 10, 'max_features': 'auto'}\n",
      "computed_metrics: [79827.27414020857]\n",
      "Model 1/8, Metric mean_absolute_error, Validation set 0: 79827.274140\n",
      "\tbest_validation_score so far: 79827.274140\n",
      "Fitting model 2/8\n",
      "hyperparameters: {'n_estimators': 10, 'max_features': 'sqrt'}\n",
      "computed_metrics: [78892.12673034341]\n",
      "Model 2/8, Metric mean_absolute_error, Validation set 1: 78892.126730\n",
      "\tbest_validation_score so far: 79827.274140\n",
      "Fitting model 3/8\n",
      "hyperparameters: {'n_estimators': 10, 'max_features': 'log2'}\n",
      "computed_metrics: [78316.09400571442]\n",
      "Model 3/8, Metric mean_absolute_error, Validation set 2: 78316.094006\n",
      "\tbest_validation_score so far: 79827.274140\n",
      "Fitting model 4/8\n",
      "hyperparameters: {'n_estimators': 10, 'max_features': None}\n",
      "computed_metrics: [82283.4597710733]\n",
      "Model 4/8, Metric mean_absolute_error, Validation set 3: 82283.459771\n",
      "\tbest_validation_score so far: 82283.459771\n",
      "Fitting model 5/8\n",
      "hyperparameters: {'n_estimators': 100, 'max_features': 'auto'}\n",
      "computed_metrics: [79629.01730728758]\n",
      "Model 5/8, Metric mean_absolute_error, Validation set 4: 79629.017307\n",
      "\tbest_validation_score so far: 82283.459771\n",
      "Fitting model 6/8\n",
      "hyperparameters: {'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "computed_metrics: [78450.08895822821]\n",
      "Model 6/8, Metric mean_absolute_error, Validation set 5: 78450.088958\n",
      "\tbest_validation_score so far: 82283.459771\n",
      "Fitting model 7/8\n",
      "hyperparameters: {'n_estimators': 100, 'max_features': 'log2'}\n",
      "computed_metrics: [77628.14930681729]\n",
      "Model 7/8, Metric mean_absolute_error, Validation set 6: 77628.149307\n",
      "\tbest_validation_score so far: 82283.459771\n",
      "Fitting model 8/8\n",
      "hyperparameters: {'n_estimators': 100, 'max_features': None}\n",
      "computed_metrics: [78097.00953371882]\n",
      "Model 8/8, Metric mean_absolute_error, Validation set 7: 78097.009534\n",
      "\tbest_validation_score so far: 82283.459771\n",
      "computed_metrics: [31763.174607498015]\n",
      "Best hyperparameters: (10, None)\n",
      "train_score: 31763.174607\n",
      "validation_score: 82283.459771\n"
     ]
    }
   ],
   "source": [
    "def rf_model_builder(model_params, model_dir):\n",
    "  sklearn_model = RandomForestRegressor(**model_params)\n",
    "  return dc.models.SklearnModel(sklearn_model, model_dir)\n",
    "params_dict = {\n",
    "    \"n_estimators\": [10, 100],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.mean_absolute_error)\n",
    "optimizer = dc.hyper.HyperparamOpt(rf_model_builder)\n",
    "best_rf, best_rf_hyperparams, all_rf_results = optimizer.hyperparam_search(\n",
    "    params_dict, train_dataset, valid_dataset, transformers,\n",
    "    metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1/1\n",
      "hyperparameters: {'kernel': 'laplacian', 'alpha': 0.0001, 'gamma': 0.0001}\n",
      "computed_metrics: [88064.61087734009]\n",
      "Model 1/1, Metric mean_absolute_error, Validation set 0: 88064.610877\n",
      "\tbest_validation_score so far: 88064.610877\n",
      "computed_metrics: [34899.127982270154]\n",
      "Best hyperparameters: ('laplacian', 0.0001, 0.0001)\n",
      "train_score: 34899.127982\n",
      "validation_score: 88064.610877\n"
     ]
    }
   ],
   "source": [
    "def krr_model_builder(model_params, model_dir):\n",
    "  sklearn_model = KernelRidge(**model_params)\n",
    "  return dc.models.SklearnModel(sklearn_model, model_dir)\n",
    "\n",
    "params_dict = {\n",
    "    \"kernel\": [\"laplacian\"],\n",
    "    \"alpha\": [0.0001],\n",
    "    \"gamma\": [0.0001]\n",
    "}\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.mean_absolute_error)\n",
    "optimizer = dc.hyper.HyperparamOpt(krr_model_builder)\n",
    "best_krr, best_krr_hyperparams, all_krr_results = optimizer.hyperparam_search(\n",
    "    params_dict, train_dataset, valid_dataset, transformers,\n",
    "    metric=metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
