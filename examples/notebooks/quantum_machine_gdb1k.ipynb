{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quantum Machinery with gdb1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb off\n",
    "__author__ = \"Joseph Gomes and Bharath Ramsundar\"\n",
    "__copyright__ = \"Copyright 2016, Stanford University\"\n",
    "__license__ = \"LGPL\"\n",
    "\n",
    "import os\n",
    "import unittest\n",
    "\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "import numpy.random\n",
    "from deepchem.utils.evaluate import Evaluator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = dc.feat.CoulombMatrixEig(23, remove_hydrogens=False)\n",
    "tasks = [\"atomization_energy\"]\n",
    "dataset_file = \"../../datasets/gdb1k.sdf\"\n",
    "smiles_field = \"smiles\"\n",
    "mol_field = \"mol\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load featurized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "Reading structures from ../../datasets/gdb1k.sdf.\n",
      "Currently featurizing feature_type: CoulombMatrixEig\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.338 s\n",
      "TIMING: dataset construction took 0.479 s\n",
      "Loading dataset from disk.\n",
      "type of diskData <class 'deepchem.data.datasets.DiskDataset'>\n"
     ]
    }
   ],
   "source": [
    "loader = dc.data.SDFLoader(\n",
    "      tasks=[\"atomization_energy\"], smiles_field=\"smiles\",\n",
    "      featurizer=featurizer,\n",
    "      mol_field=\"mol\")\n",
    "dataset = loader.featurize(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Train, Validation, and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 0.011 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.005 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.005 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "random_splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = random_splitter.train_valid_test_split(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 0.010 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.010 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.003 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.003 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.003 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.003 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "transformers = [\n",
    "    dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset),\n",
    "    dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]\n",
    "\n",
    "for dataset in [train_dataset, valid_dataset, test_dataset]:\n",
    "  for transformer in transformers:\n",
    "      dataset = transformer.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Fit Random Forest with hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1/8\n",
      "hyperparameters: {'n_estimators': 10, 'max_features': 'auto'}\n",
      "computed_metrics: [82238.16595651126]\n",
      "Model 1/8, Metric mean_absolute_error, Validation set 0: 82238.165957\n",
      "\tbest_validation_score so far: 82238.165957\n",
      "Fitting model 2/8\n",
      "hyperparameters: {'n_estimators': 10, 'max_features': 'sqrt'}\n",
      "computed_metrics: [83245.07611794364]\n",
      "Model 2/8, Metric mean_absolute_error, Validation set 1: 83245.076118\n",
      "\tbest_validation_score so far: 83245.076118\n",
      "Fitting model 3/8\n",
      "hyperparameters: {'n_estimators': 10, 'max_features': 'log2'}\n",
      "computed_metrics: [85169.93092738534]\n",
      "Model 3/8, Metric mean_absolute_error, Validation set 2: 85169.930927\n",
      "\tbest_validation_score so far: 85169.930927\n",
      "Fitting model 4/8\n",
      "hyperparameters: {'n_estimators': 10, 'max_features': None}\n",
      "computed_metrics: [85954.32998255146]\n",
      "Model 4/8, Metric mean_absolute_error, Validation set 3: 85954.329983\n",
      "\tbest_validation_score so far: 85954.329983\n",
      "Fitting model 5/8\n",
      "hyperparameters: {'n_estimators': 100, 'max_features': 'auto'}\n",
      "computed_metrics: [79237.08524102329]\n",
      "Model 5/8, Metric mean_absolute_error, Validation set 4: 79237.085241\n",
      "\tbest_validation_score so far: 85954.329983\n",
      "Fitting model 6/8\n",
      "hyperparameters: {'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "computed_metrics: [79265.18387671527]\n",
      "Model 6/8, Metric mean_absolute_error, Validation set 5: 79265.183877\n",
      "\tbest_validation_score so far: 85954.329983\n",
      "Fitting model 7/8\n",
      "hyperparameters: {'n_estimators': 100, 'max_features': 'log2'}\n",
      "computed_metrics: [79289.37641840623]\n",
      "Model 7/8, Metric mean_absolute_error, Validation set 6: 79289.376418\n",
      "\tbest_validation_score so far: 85954.329983\n",
      "Fitting model 8/8\n",
      "hyperparameters: {'n_estimators': 100, 'max_features': None}\n",
      "computed_metrics: [81321.83184235706]\n",
      "Model 8/8, Metric mean_absolute_error, Validation set 7: 81321.831842\n",
      "\tbest_validation_score so far: 85954.329983\n",
      "computed_metrics: [33530.24706055821]\n",
      "Best hyperparameters: (10, None)\n",
      "train_score: 33530.247061\n",
      "validation_score: 85954.329983\n"
     ]
    }
   ],
   "source": [
    "def rf_model_builder(model_params, model_dir):\n",
    "  sklearn_model = RandomForestRegressor(**model_params)\n",
    "  return dc.models.SklearnModel(sklearn_model, model_dir)\n",
    "params_dict = {\n",
    "    \"n_estimators\": [10, 100],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.mean_absolute_error)\n",
    "optimizer = dc.hyper.HyperparamOpt(rf_model_builder)\n",
    "best_rf, best_rf_hyperparams, all_rf_results = optimizer.hyperparam_search(\n",
    "    params_dict, train_dataset, valid_dataset, transformers,\n",
    "    metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1/1\n",
      "hyperparameters: {'kernel': 'laplacian', 'alpha': 0.0001, 'gamma': 0.0001}\n",
      "computed_metrics: [93703.78574695572]\n",
      "Model 1/1, Metric mean_absolute_error, Validation set 0: 93703.785747\n",
      "\tbest_validation_score so far: 93703.785747\n",
      "computed_metrics: [36729.00616224196]\n",
      "Best hyperparameters: ('laplacian', 0.0001, 0.0001)\n",
      "train_score: 36729.006162\n",
      "validation_score: 93703.785747\n"
     ]
    }
   ],
   "source": [
    "def krr_model_builder(model_params, model_dir):\n",
    "  sklearn_model = KernelRidge(**model_params)\n",
    "  return dc.models.SklearnModel(sklearn_model, model_dir)\n",
    "\n",
    "params_dict = {\n",
    "    \"kernel\": [\"laplacian\"],\n",
    "    \"alpha\": [0.0001],\n",
    "    \"gamma\": [0.0001]\n",
    "}\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.mean_absolute_error)\n",
    "optimizer = dc.hyper.HyperparamOpt(krr_model_builder)\n",
    "best_krr, best_krr_hyperparams, all_krr_results = optimizer.hyperparam_search(\n",
    "    params_dict, train_dataset, valid_dataset, transformers,\n",
    "    metric=metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
