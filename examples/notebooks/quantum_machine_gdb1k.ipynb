{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quantum Machinery with gdb1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "__author__ = \"Joseph Gomes and Bharath Ramsundar\"\n",
    "__copyright__ = \"Copyright 2016, Stanford University\"\n",
    "__license__ = \"LGPL\"\n",
    "\n",
    "import os\n",
    "import unittest\n",
    "\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "import numpy.random\n",
    "from deepchem.utils.evaluate import Evaluator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = dc.feat.CoulombMatrixEig(23, remove_hydrogens=False)\n",
    "tasks = [\"atomization_energy\"]\n",
    "dataset_file = \"../../datasets/gdb1k.sdf\"\n",
    "smiles_field = \"smiles\"\n",
    "mol_field = \"mol\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load featurized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "Reading structures from ../../datasets/gdb1k.sdf.\n",
      "Currently featurizing feature_type: CoulombMatrixEig\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.528 s\n",
      "TIMING: dataset construction took 0.715 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "loader = dc.data.SDFLoader(\n",
    "      tasks=[\"atomization_energy\"], smiles_field=\"smiles\",\n",
    "      featurizer=featurizer,\n",
    "      mol_field=\"mol\")\n",
    "dataset = loader.featurize(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Train, Validation, and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 0.016 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.007 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.007 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "random_splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = random_splitter.train_valid_test_split(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 0.015 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.013 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.004 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.005 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.006 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.006 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "transformers = [\n",
    "    dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset),\n",
    "    dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]\n",
    "\n",
    "for dataset in [train_dataset, valid_dataset, test_dataset]:\n",
    "  for transformer in transformers:\n",
    "      dataset = transformer.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Fit Random Forest with hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1/8\n",
      "hyperparameters: {'max_features': 'auto', 'n_estimators': 10}\n",
      "computed_metrics: [86127.09575562611]\n",
      "Model 1/8, Metric mean_absolute_error, Validation set 0: 86127.095756\n",
      "\tbest_validation_score so far: 86127.095756\n",
      "Fitting model 2/8\n",
      "hyperparameters: {'max_features': 'auto', 'n_estimators': 100}\n",
      "computed_metrics: [83531.16789342878]\n",
      "Model 2/8, Metric mean_absolute_error, Validation set 1: 83531.167893\n",
      "\tbest_validation_score so far: 86127.095756\n",
      "Fitting model 3/8\n",
      "hyperparameters: {'max_features': 'sqrt', 'n_estimators': 10}\n",
      "computed_metrics: [83769.00864289024]\n",
      "Model 3/8, Metric mean_absolute_error, Validation set 2: 83769.008643\n",
      "\tbest_validation_score so far: 86127.095756\n",
      "Fitting model 4/8\n",
      "hyperparameters: {'max_features': 'sqrt', 'n_estimators': 100}\n",
      "computed_metrics: [83750.48498142553]\n",
      "Model 4/8, Metric mean_absolute_error, Validation set 3: 83750.484981\n",
      "\tbest_validation_score so far: 86127.095756\n",
      "Fitting model 5/8\n",
      "hyperparameters: {'max_features': 'log2', 'n_estimators': 10}\n",
      "computed_metrics: [84741.39090102281]\n",
      "Model 5/8, Metric mean_absolute_error, Validation set 4: 84741.390901\n",
      "\tbest_validation_score so far: 86127.095756\n",
      "Fitting model 6/8\n",
      "hyperparameters: {'max_features': 'log2', 'n_estimators': 100}\n",
      "computed_metrics: [84648.3044189912]\n",
      "Model 6/8, Metric mean_absolute_error, Validation set 5: 84648.304419\n",
      "\tbest_validation_score so far: 86127.095756\n",
      "Fitting model 7/8\n",
      "hyperparameters: {'max_features': None, 'n_estimators': 10}\n",
      "computed_metrics: [80825.53703832021]\n",
      "Model 7/8, Metric mean_absolute_error, Validation set 6: 80825.537038\n",
      "\tbest_validation_score so far: 86127.095756\n",
      "Fitting model 8/8\n",
      "hyperparameters: {'max_features': None, 'n_estimators': 100}\n",
      "computed_metrics: [81706.31823864301]\n",
      "Model 8/8, Metric mean_absolute_error, Validation set 7: 81706.318239\n",
      "\tbest_validation_score so far: 86127.095756\n",
      "computed_metrics: [31036.05618795521]\n",
      "Best hyperparameters: ('auto', 10)\n",
      "train_score: 31036.056188\n",
      "validation_score: 86127.095756\n"
     ]
    }
   ],
   "source": [
    "def rf_model_builder(model_params, model_dir):\n",
    "  sklearn_model = RandomForestRegressor(**model_params)\n",
    "  return dc.models.SklearnModel(sklearn_model, model_dir)\n",
    "params_dict = {\n",
    "    \"n_estimators\": [10, 100],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.mean_absolute_error)\n",
    "optimizer = dc.hyper.HyperparamOpt(rf_model_builder)\n",
    "best_rf, best_rf_hyperparams, all_rf_results = optimizer.hyperparam_search(\n",
    "    params_dict, train_dataset, valid_dataset, transformers,\n",
    "    metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1/1\n",
      "hyperparameters: {'alpha': 0.0001, 'kernel': 'laplacian', 'gamma': 0.0001}\n",
      "computed_metrics: [92814.92810152171]\n",
      "Model 1/1, Metric mean_absolute_error, Validation set 0: 92814.928102\n",
      "\tbest_validation_score so far: 92814.928102\n",
      "computed_metrics: [35135.73181665828]\n",
      "Best hyperparameters: (0.0001, 'laplacian', 0.0001)\n",
      "train_score: 35135.731817\n",
      "validation_score: 92814.928102\n"
     ]
    }
   ],
   "source": [
    "def krr_model_builder(model_params, model_dir):\n",
    "  sklearn_model = KernelRidge(**model_params)\n",
    "  return dc.models.SklearnModel(sklearn_model, model_dir)\n",
    "\n",
    "params_dict = {\n",
    "    \"kernel\": [\"laplacian\"],\n",
    "    \"alpha\": [0.0001],\n",
    "    \"gamma\": [0.0001]\n",
    "}\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.mean_absolute_error)\n",
    "optimizer = dc.hyper.HyperparamOpt(krr_model_builder)\n",
    "best_krr, best_krr_hyperparams, all_krr_results = optimizer.hyperparam_search(\n",
    "    params_dict, train_dataset, valid_dataset, transformers,\n",
    "    metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
