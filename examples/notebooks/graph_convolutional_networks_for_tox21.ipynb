{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutions For Tox21\n",
    "In this notebook, we will explore the use of TensorGraph to create graph convolutional models with DeepChem. In particular, we will build a graph convolutional network on the Tox21 dataset.\n",
    "\n",
    "Let's start with some basic imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/anaconda3/envs/deepchem/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import deepchem as dc\n",
    "from deepchem.models.tensorgraph.models.graph_models import GraphConvTensorGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use MoleculeNet to load the Tox21 dataset. We need to make sure to process the data in a way that graph convolutional networks can use For that, we make sure to set the featurizer option to 'GraphConv'. The MoleculeNet call will return a training set, an validation set, and a test set for us to use. The call also returns `transformers`, a list of data transformations that were applied to preprocess the dataset. (Most deep networks are quite finicky and require a set of data transformations to ensure that training proceeds stably.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/tox21.csv.gz\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "TIMING: featurizing shard 0 took 14.856 s\n",
      "TIMING: dataset construction took 18.242 s\n",
      "Loading dataset from disk.\n",
      "About to transform data\n",
      "TIMING: dataset construction took 4.662 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 4.070 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.720 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.882 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "# Load Tox21 dataset\n",
    "tox21_tasks, tox21_datasets, transformers = dc.molnet.load_tox21(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = tox21_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a graph convolutional network on this dataset. DeepChem has the class `GraphConvTensorGraph` that wraps a standard graph convolutional architecture underneath the hood for user convenience. Let's instantiate an object of this class and train it on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n",
      "Starting epoch 6\n",
      "Starting epoch 7\n",
      "Ending global_step 999: Average loss 479.205\n",
      "Starting epoch 8\n",
      "Starting epoch 9\n",
      "Ending global_step 1260: Average loss 385.126\n",
      "TIMING: model fitting took 96.576 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385.1264790794402"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GraphConvTensorGraph(\n",
    "    len(tox21_tasks), batch_size=50, mode='classification')\n",
    "# Set nb_epoch=10 for better results.\n",
    "model.fit(train_dataset, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to evaluate the performance of the model we've trained. For this, we need to define a metric, a measure of model performance. `dc.metrics` holds a collection of metrics already. For this dataset, it is standard to use the ROC-AUC score, the area under the receiver operating characteristic curve (which measures the tradeoff between precision and recall). Luckily, the ROC-AUC score is already available in DeepChem. \n",
    "\n",
    "To measure the performance of the model under this metric, we can use the convenience function `model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.906771561997548, 0.9588769970295441, 0.9066302543268427, 0.9279185292493163, 0.8083711042403153, 0.9109737117028761, 0.9603608223832099, 0.8380452411438326, 0.9348178803337903, 0.8874621376403034, 0.9141104838995989, 0.9056760082787939]\n",
      "Training ROC-AUC Score: 0.905001\n",
      "computed_metrics: [0.8161579181714738, 0.824156746031746, 0.8631047194283894, 0.8366166751398068, 0.7138636363636364, 0.7569428054441036, 0.8332467982000692, 0.8551633434322083, 0.8464798681422181, 0.7875539374325782, 0.8784641548300558, 0.8434969853574505]\n",
      "Validation ROC-AUC Score: 0.821271\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_scores = model.evaluate(train_dataset, [metric], transformers)\n",
    "print(\"Training ROC-AUC Score: %f\" % train_scores[\"mean-roc_auc_score\"])\n",
    "valid_scores = model.evaluate(valid_dataset, [metric], transformers)\n",
    "print(\"Validation ROC-AUC Score: %f\" % valid_scores[\"mean-roc_auc_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on under the hood? Could we build `GraphConvTensorGraph` ourselves? Of course! The first step is to create a `TensorGraph` object. This object will hold the \"computational graph\" that defines the computation that a graph convolutional network will perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.tensor_graph import TensorGraph\n",
    "\n",
    "tg = TensorGraph(use_queue=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the inputs to our model. Conceptually, graph convolutions just requires a the structure of the molecule in question and a vector of features for every atom that describes the local chemical environment. However in practice, due to TensorFlow's limitations as a general programming environment, we have to have some auxiliary information as well preprocessed.\n",
    "\n",
    "`atom_features` holds a feature vector of length 75 for each atom. The other feature inputs are required to support minibatching in TensorFlow. `degree_slice` is an indexing convenience that makes it easy to locate atoms from all molecules with a given degree. `membership` determines the membership of atoms in molecules (atom `i` belongs to molecule `membership[i]`). `deg_adjs` is a list that contains adjacency lists grouped by atom degree For more details, check out the [code](https://github.com/deepchem/deepchem/blob/master/deepchem/feat/mol_graphs.py).\n",
    "\n",
    "To define feature inputs in `TensorGraph`, we use the `Feature` layer. Conceptually, a `TensorGraph` is a mathematical graph composed of layer objects. `Features` layers have to be the root nodes of the graph since they consitute inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Feature\n",
    "\n",
    "atom_features = Feature(shape=(None, 75))\n",
    "degree_slice = Feature(shape=(None, 2), dtype=tf.int32)\n",
    "membership = Feature(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "deg_adjs = []\n",
    "for i in range(0, 10 + 1):\n",
    "    deg_adj = Feature(shape=(None, i + 1), dtype=tf.int32)\n",
    "    deg_adjs.append(deg_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the body of the graph convolutional network. `TensorGraph` has a number of layers that encode various graph operations. Namely, the `GraphConv`, `GraphPool` and `GraphGather` layers. We will also apply standard neural network layers such as `Dense` and `BatchNorm`.\n",
    "\n",
    "The layers we're adding effect a \"feature transformation\" that will create one vector for each molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Dense, GraphConv, BatchNorm\n",
    "from deepchem.models.tensorgraph.layers import GraphPool, GraphGather\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "gc1 = GraphConv(\n",
    "    64,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    in_layers=[atom_features, degree_slice, membership] + deg_adjs)\n",
    "batch_norm1 = BatchNorm(in_layers=[gc1])\n",
    "gp1 = GraphPool(in_layers=[batch_norm1, degree_slice, membership] + deg_adjs)\n",
    "gc2 = GraphConv(\n",
    "    64,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    in_layers=[gp1, degree_slice, membership] + deg_adjs)\n",
    "batch_norm2 = BatchNorm(in_layers=[gc2])\n",
    "gp2 = GraphPool(in_layers=[batch_norm2, degree_slice, membership] + deg_adjs)\n",
    "dense = Dense(out_channels=128, activation_fn=tf.nn.relu, in_layers=[gp2])\n",
    "batch_norm3 = BatchNorm(in_layers=[dense])\n",
    "readout = GraphGather(\n",
    "    batch_size=batch_size,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    in_layers=[batch_norm3, degree_slice, membership] + deg_adjs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make predictions from the `TensorGraph` model. Tox21 is a multitask dataset. That is, there are 12 different datasets grouped together, which share many common molecules, but with different outputs for each. As a result, we have to add a separate output layer for each task. We will use a `for` loop over the `tox21_tasks` list to make this happen. We need to add labels for each\n",
    "\n",
    "We also have to define a loss for the model which tells the network the objective to minimize during training.\n",
    "\n",
    "We have to tell `TensorGraph` which layers are outputs with `TensorGraph.add_output(layer)`. Similarly, we tell the network its loss with `TensorGraph.set_loss(loss)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Dense, SoftMax, \\\n",
    "    SoftMaxCrossEntropy, WeightedError, Stack\n",
    "from deepchem.models.tensorgraph.layers import Label, Weights\n",
    "\n",
    "costs = []\n",
    "labels = []\n",
    "for task in range(len(tox21_tasks)):\n",
    "    classification = Dense(\n",
    "        out_channels=2, activation_fn=None, in_layers=[readout])\n",
    "\n",
    "    softmax = SoftMax(in_layers=[classification])\n",
    "    tg.add_output(softmax)\n",
    "\n",
    "    label = Label(shape=(None, 2))\n",
    "    labels.append(label)\n",
    "    cost = SoftMaxCrossEntropy(in_layers=[label, classification])\n",
    "    costs.append(cost)\n",
    "all_cost = Stack(in_layers=costs, axis=1)\n",
    "weights = Weights(shape=(None, len(tox21_tasks)))\n",
    "loss = WeightedError(in_layers=[all_cost, weights])\n",
    "tg.set_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've successfully defined our graph convolutional model in `TensorGraph`, we need to train it. We can call `fit()`, but we need to make sure that each minibatch of data populates all four `Feature` objects that we've created. For this, we need to create a Python generator that given a batch of data generates a dictionary whose keys are the `Feature` layers and whose values are Numpy arrays we'd like to use for this step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol\n",
    "\n",
    "def data_generator(dataset, epochs=1, predict=False, pad_batches=True):\n",
    "  for epoch in range(epochs):\n",
    "    if not predict:\n",
    "        print('Starting epoch %i' % epoch)\n",
    "    for ind, (X_b, y_b, w_b, ids_b) in enumerate(\n",
    "        dataset.iterbatches(\n",
    "            batch_size, pad_batches=pad_batches, deterministic=True)):\n",
    "      d = {}\n",
    "      for index, label in enumerate(labels):\n",
    "        d[label] = to_one_hot(y_b[:, index])\n",
    "      d[weights] = w_b\n",
    "      multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
    "      d[atom_features] = multiConvMol.get_atom_features()\n",
    "      d[degree_slice] = multiConvMol.deg_slice\n",
    "      d[membership] = multiConvMol.membership\n",
    "      for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "        d[deg_adjs[i - 1]] = multiConvMol.get_deg_adjacency_lists()[i]\n",
    "      yield d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the model using `TensorGraph.fit_generator(generator)` which will use the generator we've defined to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Ending global_step 126: Average loss 589.316\n",
      "TIMING: model fitting took 11.731 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "589.3160233270554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Epochs set to 1 to render tutorials online.\n",
    "# Set epochs=10 for better results.\n",
    "tg.fit_generator(data_generator(train_dataset, epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our graph convolutional method, let's evaluate its performance. We again have to use our defined generator to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.8099206218995261, 0.8378020632626846, 0.8248802663674919, 0.7618759352729038, 0.6780109600274832, 0.7960194443852205, 0.781699496846203, 0.6991147598604103, 0.7766974433301086, 0.6814282740315389, 0.7715778859561007, 0.7442003691894613]\n",
      "Training ROC-AUC Score: 0.763602\n",
      "computed_metrics: [0.7613911838915549, 0.7476851851851852, 0.7948846311382151, 0.7376715810879513, 0.625409090909091, 0.6808669656203288, 0.6176877812391831, 0.722327908123723, 0.7307864374852837, 0.5634439050701187, 0.7380057007353469, 0.6999138673557279]\n",
      "Valid ROC-AUC Score: 0.701673\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "def reshape_y_pred(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    TensorGraph.Predict returns a list of arrays, one for each output\n",
    "    We also have to remove the padding on the last batch\n",
    "    Metrics taks results of shape (samples, n_task, prob_of_class)\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    retval = np.stack(y_pred, axis=1)\n",
    "    return retval[:n_samples]\n",
    "    \n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_predictions = tg.predict_on_generator(data_generator(train_dataset, predict=True))\n",
    "train_predictions = reshape_y_pred(train_dataset.y, train_predictions)\n",
    "train_scores = metric.compute_metric(train_dataset.y, train_predictions, train_dataset.w)\n",
    "print(\"Training ROC-AUC Score: %f\" % train_scores)\n",
    "\n",
    "valid_predictions = tg.predict_on_generator(data_generator(valid_dataset, predict=True))\n",
    "valid_predictions = reshape_y_pred(valid_dataset.y, valid_predictions)\n",
    "valid_scores = metric.compute_metric(valid_dataset.y, valid_predictions, valid_dataset.w)\n",
    "print(\"Valid ROC-AUC Score: %f\" % valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The model we've constructed behaves nearly identically to `GraphConvTensorGraph`. If you're looking to build your own custom models, you can follow the example we've provided here to do so. We hope to see exciting constructions from your end soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
