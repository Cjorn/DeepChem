{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Equivariant NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Karan Bania](https://karannb.github.io/)\n",
    "\n",
    "Table of Contents:\n",
    "- [Introduction to Equivariance](#introduction-to-equivariance)\n",
    "- [Equivariant Neural Networks](#equivariant-neural-networks)\n",
    "- [Phonon Prediction using E3NNs](#phonon-prediction-using-e3nns)\n",
    "\n",
    "< Write intro >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial let us consider the following two symmetries in the geometry of our input, **Rotations and Inversions (or parity)**.\n",
    "By inversion, we mean how a particular object is transformed upon reflection.\n",
    "\n",
    "We will also limit our discussion to learning (equivariant) polynomial functions, because:\n",
    "1. They are easy to deal with,\n",
    "2. They are very powerful and can approximate any function to arbitrary precision,\n",
    "3. There is a rich source of theory backing them up.\n",
    "\n",
    "(We will consider any $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ as a ploynomial map)\n",
    "\n",
    "These equivariances are present everywhere in crystal lattices and can help us learn very powerful representations of the data.\n",
    "(The two symmetries combined are included in a special group called **O(3)**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irreducible Representations (Irreps)\n",
    "To build such networks we need to also understand irreps.\n",
    "Equivariant polynomials are closed under composition, addtion and cartesian product, i.e. result in an equivariant polynomial when combined using any of these operations.\n",
    "Yet, something interesting happens when we consider the cartesian product.\n",
    "\n",
    "Given two equivariant polynomial functions, $h$ and $f$, acting on $V$ and $V'$ respectively, with $D(g)$ and $D'(g)$ as the transformations, the function $h \\otimes f$ is also equivariant, with $x \\otimes y$ being transformed by $D(g) \\otimes D'(g)$, i.e., \n",
    "$$\n",
    "h(D(g)x) \\otimes f(D'(g)y) = (D(g) \\otimes D'(g))(h \\otimes f)(x \\otimes y)\n",
    "$$\n",
    "\n",
    "Note however, that $dim(D(g) \\otimes D'(g)) = dim(D(g))*dim(D'(g))$, i.e., the dimensionality of the final representation is the **product** of the dimensions of the two representations.\n",
    "This growth of dimensionality is problematic, and we need to find a way to **reduce** this dimensionality.\n",
    "Also compositions are **necessary** to develop expressive polynomial functions, so this is not just out of mathematical curiosity.\n",
    "\n",
    "**Irreps** are the solution to this problem.\n",
    "Basically, it is possible to decompose a $x \\otimes y$ where $x \\in V$ and $y \\in V'$ into a sum of **irreducible** representations, as follows - \n",
    "$$\n",
    "x \\otimes y = |d1 - d2| \\oplus \\ldots \\oplus (d1 + d2)\n",
    "$$\n",
    "i.e., a sum of vectors with dimenionality ranging from $|d1 - d2|$ to $(d1 + d2)$; where $d1$ and $d2$ are the dimensions of the two representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E3 Equivariant NNs\n",
    "With this we can finally build our equivariant neural network layer!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
