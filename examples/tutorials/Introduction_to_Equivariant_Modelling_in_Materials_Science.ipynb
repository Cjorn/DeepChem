{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Equivariant Modelling in Materials Science\n",
    "\n",
    "Author: \n",
    "\n",
    "Table of contents:\n",
    "- [Why Equivariance?](#why-equivariance)\n",
    "- [Symmetry in Materials Science](#symmetry-in-materials-science)\n",
    "- [What is Equivariance?](#what-is-equivariance)\n",
    "- [Building Equivariant NNs](#building-equivariant-nns)\n",
    "\n",
    "In this tutorial we will explore the basics of equivariance (with some math) and also a concrete example of using Neural Networks equipped with equivariant layers. \n",
    "No mathematical background is required, we will also provide references for further reading in case you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Equivariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should we use equivariance in the first place?\n",
    "- The math involved is quite advanced,\n",
    "- These methods are tough to implement,\n",
    "- A unified implementation framework is also absent, though <a href=\"https://e3nn.org/\">e3nn</a> has come quite far to solving this problem.\n",
    "\n",
    "Despite these, it is encouraging to note that methods using equivariance are currently State-of-the-art (/ only models to do the task with a reasonable performance!) in many computational benchmarks with Materials Science applications, for e.g., Phonon Prediction [Fang et. al, 2024](https://doi.org/10.48550/arXiv.2403.11347), Phonons are quantized modes of vibrations in a crsytal lattice, and are important in understanding the thermal properties of materials; or Charge Density Prediction [Fu et. al, 2024](https://doi.org/10.48550/arXiv.2405.19276), charge density is essential to desnity functional theory and is used to derive *all* chemical properties of a material.\n",
    "Such networks have also been used for generating unique 3D molecular structures ([Daigvane et. al, 2023](https://doi.org/10.48550/arXiv.2311.16199)).\n",
    "\n",
    "<center>\n",
    "\n",
    "| Metric (&uarr;) | Symphony (Uses Equivariancs) | G-SchNet (Does not use Equivariance) | G-SphereNet (Does not use Equivariance) |\n",
    "| :--------------: | :--------------------------: | :----------------------------------: | :------------------------------------: |\n",
    "| Validity using xyz2mol(%) | **83.5** | 74.97 | 26.92 |\n",
    "| Uniqueness using xyz2mol(%) | **97.8** | 96.73 | 21.69 |\n",
    "\n",
    "Results from Daigvane et. al, 2023 showing the superiority of Symphony over other methods in generating unique 3D molecular structures.\n",
    "</center>\n",
    "\n",
    "Apart from these explicit high-level results, Equivariant modelling is also highly data-efficient and also scales well compared to other methods.\n",
    "We will explore these two points in some detail in the following sections, but these two advantages are very important, because data for Materials Science applications is often scarce and expensive to generate, and the computational cost of running simulations is very high.\n",
    "Using equivariant models can help us save on both these fronts.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Method | NMAE[\\%] (&darr;) | Molecules/min. |\n",
    "| :----: | :---------------: | :------------: |\n",
    "| ChargE3Net (Partially Equivariant) | 0.196 ± 0.001 | 3.95 |\n",
    "| InfGCN (Invariant) | 0.869 ± 0.002 | 72.00 |\n",
    "| Best eSCN (Fully Equivariant) | **0.178 ± 0.001** | **125.29** |\n",
    "\n",
    "Results from Fu et. al, 2024 showing the superiority and efficiency of their networks eSCN over other methods in predicting charge density.\n",
    "</center>\n",
    "\n",
    "With the proliferation of this knowledge, these models are now also being applied to Protein Folding ([AlphaFold-3](https://doi.org/10.1038/s41586-024-07487-w) which is again State-of-the-art (SOTA) on multiple benchmarks), and Materials Exploration / Generation on scale ([GNoME](https://doi.org/10.1038/s41586-023-06735-9)), these methods have equivariance at the very heart of their models.\n",
    "Predictions from GNoME are also being used to synthesize materials with *specific properties*, which is a testament to the power of these methods.\n",
    "\n",
    "<p float=\"left\" align=\"center\">\n",
    "  <img src=\"assets/Intro-To-Equi-tutorial/AF3-generated-molecule.png\" align=\"center\" width=\"450\"/>\n",
    "  <img src=\"assets/Intro-To-Equi-tutorial/GNoME.png\" align=\"center\" width=\"400\"/>\n",
    "  <center> <i>Left</i>: A molecule generated by AlphaFold-3, <i>Right</i>: Impact of GNoME on Materials Discovery. </center>\n",
    "</p>\n",
    "\n",
    "Finally, we would also like to point out the methods involved are very interesting as they **supply** the model with information on **how** we represent 3D structures in real life and definitely warrant a read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symmetry in Materials Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further the discussion on \"Why?\", we might just ask, why use any symmetry preserving methods at all?\n",
    "\n",
    "To answer this, we need to understand that **we** model the physical world around us using coordinate systems and Euclidean Geometry, but, these are **non-existent** in the real world! \n",
    "There is no universal reference frame.\n",
    "As such, we have already biased our models to a particular view, and we would like to remove these biases.\n",
    "In certain applications, the effect of these biases is not pronounced, for example, if you have ever worked with images, we can make the model unbiased / **invariant** to rotations by simply rotating a few training images (or sometimes even this is not necessary).\n",
    "Informally, this happens because the \"number\" of symmetries in the data is small, and we can easily account for them by augmenting the data.\n",
    "However, in our case, making a model invariant to all possible rotations of a molecule would require us to do a massive amount of augmentation, about **500x**.\n",
    "Thus, our use-case necessiates the use of some form of symmetry aware representations and/or models.\n",
    "\n",
    "To be concrete, let's consider the example of a molecule.\n",
    "Suppose we create a model to predict some property, say the charge of a Methane Molecule, based on it's 3D coordinates (might be a crappy model but atleast it will do something)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a methane molecule\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Create a methane molecule from SMILES\n",
    "methane_smiles = \"C\"\n",
    "methane_molecule = Chem.MolFromSmiles(methane_smiles)\n",
    "# Add hydrogens to the molecule\n",
    "methane_molecule = Chem.AddHs(methane_molecule)\n",
    "# Generate 3D coordinates\n",
    "AllChem.EmbedMolecule(methane_molecule)\n",
    "# Get the conformer to access 3D coordinates\n",
    "conformer = methane_molecule.GetConformer()\n",
    "# Extract coordinates into a NumPy array\n",
    "num_atoms = methane_molecule.GetNumAtoms()\n",
    "mol = np.zeros((num_atoms, 3))\n",
    "for i in range(num_atoms):\n",
    "    position = conformer.GetAtomPosition(i)\n",
    "    mol[i] = [position.x, position.y, position.z]\n",
    "\n",
    "# Display the Coordinates\n",
    "print(\"Methane coordinates (in Ångströms):\")\n",
    "print(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.randn(3)\n",
    "print(\"Random weights: {}\".format(weights))\n",
    "\n",
    "def nn(mol, weights):\n",
    "    return np.dot(mol, weights.T)\n",
    "\n",
    "# do a forward pass\n",
    "output = nn(mol, weights)\n",
    "print(\"Output: {}\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we rotate our molecule by 90 degrees about the x-axis, the charge of the molecule should not change, but our model will predict a different charge.\n",
    "And although right now the weights are random, no matter how much data we train on, our model, by design, will always predict a different charge for the rotated molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# create a rotation matrix that rotates 90 degrees around the x-axis\n",
    "r = R.from_rotvec(np.pi/2 * np.array([1, 0, 0]))\n",
    "# rotate the molecule\n",
    "mol_rotated = r.apply(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original and rotated molecule\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Original molecule\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(mol[0, 0], mol[0, 1], mol[0, 2], c='k', s=100, label='C')\n",
    "ax1.scatter(mol[1:, 0], mol[1:, 1], mol[1:, 2], c='r', s=50, label='H')\n",
    "for i in range(1, 5):\n",
    "    ax1.plot([mol[0, 0], mol[i, 0]], \n",
    "             [mol[0, 1], mol[i, 1]], \n",
    "             [mol[0, 2], mol[i, 2]], 'k-')\n",
    "ax1.set_title('Original Methane')\n",
    "ax1.legend()\n",
    "\n",
    "# Rotated molecule\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(mol_rotated[0, 0], mol_rotated[0, 1], mol_rotated[0, 2], c='k', s=100, label='C')\n",
    "ax2.scatter(mol_rotated[1:, 0], mol_rotated[1:, 1], mol_rotated[1:, 2], c='b', s=50, label='H')\n",
    "for i in range(1, 5):\n",
    "    ax2.plot([mol_rotated[0, 0], mol_rotated[i, 0]], \n",
    "             [mol_rotated[0, 1], mol_rotated[i, 1]], \n",
    "             [mol_rotated[0, 2], mol_rotated[i, 2]], 'k-')\n",
    "ax2.set_title('Rotated Methane')\n",
    "ax2.legend()\n",
    "\n",
    "# Set equal aspect ratio\n",
    "ax1.set_box_aspect((1,1,1))\n",
    "ax2.set_box_aspect((1,1,1))\n",
    "ax1.set_axis_off()\n",
    "ax2.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rotated = nn(mol_rotated, weights)\n",
    "print(\"Rotated Output: {}\".format(output_rotated))\n",
    "\n",
    "print(\"Difference in output: {}\".format(output - output_rotated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example let's try to make a model that is invariant to rotations about the x-axis. Simply enough we just have to sum across the varying axes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = np.random.randn(num_atoms)\n",
    "\n",
    "def invariant_nn(mol, weights):\n",
    "    mol_ys = np.sum(mol[:, 1])\n",
    "    mol_zs = np.sum(mol[:, 2])\n",
    "    out = np.dot(mol[:, 0], weights.T) + mol_ys + mol_zs\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_invariant = invariant_nn(mol, new_weights)\n",
    "output_rotated_invariant = invariant_nn(mol_rotated, new_weights)\n",
    "\n",
    "print(\"Output: {}\".format(output_invariant))\n",
    "print(\"Rotated Output: {}\".format(output_rotated_invariant))\n",
    "print(\"Difference in output: {}\".format(output_invariant - output_rotated_invariant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, our model is now invariant to rotations about the x-axis, but what about the y-axis / z-axis? \n",
    "What about translations? \n",
    "What about reflections?\n",
    "We can build a general model that is **invariant** to all these transformations, and we would direct the interested reader to [Ch.9 Input Data and Equivariances](https://dmol.pub/dl/data.html) for a detailed discussion on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Equivariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, equivariance is a form of symmetry for functions.\n",
    "Equivariance is easy to define and understand once we know what a **group** is and what a **representation** is.\n",
    "This section involves some math but we will try to keep it as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group\n",
    "Intuitively a group consists of transformations and decides how they **compose and interact** with each other.\n",
    "Formally, we say that a set of transformations $G$ is a group if:\n",
    "- There exists an identity element $e \\in G$ such that $eg = ge = g$ for all $g \\in G$.\n",
    "- For all $g \\in G$, there exists an inverse $g^{-1}$ such that $gg^{-1} = g^{-1}g = e$.\n",
    "- The composition of two transformations is also an transformation, i.e., for all $g, h \\in G$, $gh \\in G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation\n",
    "This is a little more abstract than the notion of a group. \n",
    "The basis behind representation theory is to allow us to do abstract algebra (e.g. Matrix Multiplication) on some vector space. \n",
    "Thus a representation exists only in context of a group **and** a vector space.\n",
    "A representation is like defining the **data-types**, it decides how a particular group element transforms a particular **type** of vector.\n",
    "Formally, the representation of a group $G$ **on** a vector space $V$, written as $D(g, x)$ follows the following rules:\n",
    "- Linearity, $D(g, x + y) = D(g, x) + D(g, y)$.\n",
    "- Commutativity, $D(g, D(h, x)) = D(gh, x)$.\n",
    "\n",
    "where $x, y \\in V$ and $g, h \\in G$.\n",
    "\n",
    "However, we usually view the transformations in a group as matrices, so we also have this equivalent notation, written as $D(g) \\times x$, such that:\n",
    "- $D(g): V \\rightarrow V$, $D(g) \\in \\mathbb{R}^{n \\times n}$, where $n$ is the dimension of $V$.\n",
    "- $D(g)D(h) = D(gh)$.\n",
    "\n",
    "Let's take an example to understand these two concepts better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose our group consists of rotations of arbitrary rotations\n",
    "# and our space is the Euclidean 3D space, \n",
    "# this is a valid group-and-vector-space combination\n",
    "\n",
    "# create a rotation matrix\n",
    "alpha, beta, gamma = np.random.rand(3) * 2 * np.pi\n",
    "r = R.from_euler('xyz', [alpha, beta, gamma]) # D(g)\n",
    "\n",
    "vec = np.random.randn(3) # v\n",
    "\n",
    "vec_ = r.apply(vec) # D(g)v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a really simple example, however, it can get complicated if the representations are not just plain numbers!\n",
    "For example, suppose we have a 2 atom system where we know the masses of the atoms and their velocities.\n",
    "We couple this information into a single 8 dimensional vector ($[m_1, m_2, v_{1x}, v_{1y}, v_{1z}, v_{2x}, v_{2y}, v_{2z}] \\in \\mathbb{R}^8$).\n",
    "Now, our system description is **still** in 3D but our representation is now 8D.\n",
    "\n",
    "Also, we know that masses don't change under rotations (scalars, a particular **type of representation**), but velocities do (vector, another type of representation).\n",
    "For now we also assume these two velocities are independent of each other (in practice this is not the case).\n",
    "When we build a model, We need to encode this information to the model in some way.\n",
    "\n",
    "Let's build a rotation matrix that obeys these properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_block = np.array([1]) # sclars don't change\n",
    "m2_block = np.array([1])\n",
    "v1_block = r # independent rotations\n",
    "v2_block = r\n",
    "\n",
    "R_prime = np.block([\n",
    "    [m1_block, np.zeros((1, 1)), np.zeros((1, 3)), np.zeros((1, 3))],\n",
    "    [np.zeros((1, 1)), m2_block, np.zeros((1, 3)), np.zeros((1, 3))],\n",
    "    [np.zeros((3, 1)), np.zeros((3, 1)), v1_block, np.zeros((3, 3))],\n",
    "    [np.zeros((3, 1)), np.zeros((3, 1)), np.zeros((3, 3)), v2_block]\n",
    "])\n",
    "\n",
    "print(R_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a block diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivariance\n",
    "\n",
    "Now we are ready to define equivariance.\n",
    "Again, formally, given a function $f: V \\rightarrow V'$, a group element $g \\in G$, representations $D$ and $D'$ of $G$ on $V$ and $V'$ respectively, we say that $f$ is equivariant to $G$ if for all $x \\in V$, we have \n",
    "$$\n",
    "f(D(g)x) = D'(g)f(x)\n",
    "$$\n",
    "\n",
    "That was really abstract, let's break it down.\n",
    "Intuitively, we want our function to act on the transformed input in the same way as the original input.\n",
    "\n",
    "But this is still quite abstract, consider this example to understand it better. \n",
    "Suppose we are working with 3D coordinates and $g$ just cycles the coordinates clockwise (permutation).\n",
    "Then \n",
    "$$f([x, y, z]) = [z + y, x + z, x + y]$$ \n",
    "is equivarant to $g$, because \n",
    "$$f(g[x, y, z]) = f([z, x, y]) = [x + y, y + z, z + x]$$\n",
    "$$g' f([x, y, z]) = g' [z + y, x + z, x + y] = [x + y, y + z, z + x]$$\n",
    "\n",
    "where $g' = g$. (This is the same math that makes GNN layers permutationally equivariant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: np.ndarray) -> np.ndarray:\n",
    "    y = [x[1] + x[2], x[0] + x[2], x[0] + x[1]]\n",
    "    return np.array(y)\n",
    "\n",
    "v1 = np.array([1, 2, 3])\n",
    "print(f\"Vector (v1): {v1}\")\n",
    "permutation = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]]) # Permutation matrix\n",
    "print(f\"Permutation matrix (g): \\n{permutation}\")\n",
    "permuted_v1 = np.dot(permutation, v1) # Permutes the vector\n",
    "print(f\"Permuted vector (g*v1): {permuted_v1}\")\n",
    "out_permuted_f1 = f(permuted_v1)\n",
    "print(f\"Output on permuted vector f1(g*v1): {out_permuted_f1}\")\n",
    "out_f1 = f(v1)\n",
    "permuted_out_f1 = np.dot(permutation, out_f1)\n",
    "print(f\"Output on vector g'f1(v1): {permuted_out_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in principle we could just use these simple transformations to build equivariant models, but in practice, we have additional information, i.e., our inputs are limited to the Euclidean Equivariant Group.\n",
    "We also need to make these learnable in some way.\n",
    "This is where Equivariant Neural Networks come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Equivariant NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial let us consider the following two symmetries in the geometry of our input, **Rotations and Inversions (or parity)**.\n",
    "By inversion, we mean how a particular object is transformed upon reflection.\n",
    "\n",
    "We will also limit our discussion to learning (equivariant) polynomial functions, because:\n",
    "1. They are easy to deal with,\n",
    "2. They are very powerful and can approximate any function to arbitrary precision,\n",
    "3. There is a rich source of theory backing them up.\n",
    "\n",
    "(We will consider any $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ as a ploynomial map)\n",
    "\n",
    "These equivariances are present everywhere in crystal lattices and can help us learn very powerful representations of the data.\n",
    "(The two symmetries combined are included in a special group called **O(3)**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irreducible Representations (Irreps)\n",
    "To build such networks we need to also understand irreps.\n",
    "Equivariant polynomials are closed under composition, addtion and cartesian product, i.e. result in an equivariant polynomial when combined using any of these operations.\n",
    "Yet, something interesting happens when we consider the cartesian product.\n",
    "\n",
    "Given two equivariant polynomial functions, $h$ and $f$, acting on $V$ and $V'$ respectively, with $D(g)$ and $D'(g)$ as the transformations, the function $h \\otimes f$ is also equivariant, with $x \\otimes y$ being transformed by $D(g) \\otimes D'(g)$, i.e., \n",
    "$$\n",
    "h(D(g)x) \\otimes f(D'(g)y) = (D(g) \\otimes D'(g))(h \\otimes f)(x \\otimes y)\n",
    "$$\n",
    "\n",
    "Note however, that $dim(D(g) \\otimes D'(g)) = dim(D(g))*dim(D'(g))$, i.e., the dimensionality of the final representation is the **product** of the dimensions of the two representations.\n",
    "This growth of dimensionality is problematic, and we need to find a way to **reduce** this dimensionality.\n",
    "Also compositions are **necessary** to develop expressive polynomial functions, so this is not just out of mathematical curiosity.\n",
    "\n",
    "**Irreps** are the solution to this problem.\n",
    "Basically, it is possible to decompose a $x \\otimes y$ where $x \\in V$ and $y \\in V'$ into a sum of **irreducible** representations, as follows - \n",
    "$$\n",
    "x \\otimes y = |d1 - d2| \\oplus \\ldots \\oplus (d1 + d2)\n",
    "$$\n",
    "i.e., a sum of vectors with dimenionality ranging from $|d1 - d2|$ to $(d1 + d2)$; where $d1$ and $d2$ are the dimensions of the two representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E3 Equivariant NNs\n",
    "With this we can finally build our equivariant neural network layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
