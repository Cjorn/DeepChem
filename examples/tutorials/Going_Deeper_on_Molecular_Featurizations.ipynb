{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTuYGOlnh117"
      },
      "source": [
        "#  Advanced Model Training\n",
        "\n",
        "In the tutorials so far we have followed a simple procedure for training models: load a dataset, create a model, call `fit()`, evaluate it, and call ourselves done.  That's fine for an example, but in real machine learning projects the process is usually more complicated.  In this tutorial we will look at a more realistic workflow for training a model.\n",
        "\n",
        "## Colab\n",
        "\n",
        "This tutorial and the rest in this sequence can be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Advanced_Model_Training.ipynb)\n",
        "\n",
        "## Setup\n",
        "\n",
        "To run DeepChem within Colab, you'll need to run the following installation commands. You can of course run this tutorial locally if you prefer. In that case, don't run these cells since they will download and install DeepChem in your local machine again."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq\n",
        "!pip install --pre deepchem"
      ],
      "metadata": {
        "id": "VhbWTw2BNr83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D43MbibL_EK0"
      },
      "outputs": [],
      "source": [
        "import deepchem\n",
        "deepchem.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omxBgQVDh12B"
      },
      "source": [
        "## Hyperparameter Optimization\n",
        "\n",
        "Let's start by loading the HIV dataset.  It classifies over 40,000 molecules based on whether they inhibit HIV replication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp5Hbb4nh12C",
        "outputId": "24dc1002-f6fb-44bb-daa9-c8dd3708bfc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.molnet.load_function.molnet_loader:'split' is deprecated.  Use 'splitter' instead.\n",
            "[19:35:33] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:35:33] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:36:00] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:36:00] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        }
      ],
      "source": [
        "import deepchem as dc\n",
        "\n",
        "tasks, datasets, transformers = dc.molnet.load_hiv(featurizer='ECFP', split='scaffold')\n",
        "train_dataset, valid_dataset, test_dataset = datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rnDKzzoGWoA"
      },
      "source": [
        "Now let's train a model on it.  We will use a `MultitaskClassifier`, which is just a stack of dense layers.  But that still leaves a lot of options.  How many layers should there be, and how wide should each one be?  What dropout rate should we use?  What learning rate?\n",
        "\n",
        "These are called hyperparameters.  The standard way to select them is to try lots of values, train each model on the training set, and evaluate it on the validation set.  This lets us see which ones work best.\n",
        "\n",
        "You could do that by hand, but usually it's easier to let the computer do it for you.  DeepChem provides a selection of hyperparameter optimization algorithms, which are found in the `dc.hyper` package.  For this example we'll use `GridHyperparamOpt`, which is the most basic method.  We just give it a list of options for each hyperparameter and it exhaustively tries all combinations of them.\n",
        "\n",
        "The lists of options are defined by a `dict` that we provide.  For each of the model's arguments, we provide a list of values to try.  In this example we consider three possible sets of hidden layers: a single layer of width 500, a single layer of width 1000, or two layers each of width 1000.  We also consider two dropout rates (20% and 50%) and two learning rates (0.001 and 0.0001)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0C2CDR0XGWoA"
      },
      "outputs": [],
      "source": [
        "#This block of code needs to be worked on, for some reason metric results to an error\n",
        "\n",
        "# import numpy as np\n",
        "# params_dict = {\n",
        "#     'n_tasks': [len(tasks)],\n",
        "#     'n_features': [1024],\n",
        "#     'layer_sizes': [[500], [1000], [1000, 1000]],\n",
        "#     'dropouts': [0.2, 0.5],\n",
        "#     'learning_rate': [0.001, 0.0001]\n",
        "# }\n",
        "# optimizer = dc.hyper.GridHyperparamOpt(dc.models.MultitaskClassifier)\n",
        "# metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
        "# best_model, best_hyperparams, all_results = optimizer.hyperparam_search(params_dict, train_dataset, valid_dataset, metric, transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qseLJMClGWoB"
      },
      "source": [
        "`hyperparam_search()` returns three arguments: the best model it found, the hyperparameters for that model, and a full listing of the validation score for every model.  Let's take a look at the last one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pvSOFwdoGWoC",
        "outputId": "4b449c3e-abbb-40dc-ffe2-8980acd55937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_dropouts_0.200000_layer_sizes[500]_learning_rate_0.001000_n_features_1024_n_tasks_1': 0.7747801538310797,\n",
              " '_dropouts_0.200000_layer_sizes[500]_learning_rate_0.000100_n_features_1024_n_tasks_1': 0.7711961713697826,\n",
              " '_dropouts_0.500000_layer_sizes[500]_learning_rate_0.001000_n_features_1024_n_tasks_1': 0.7770781280619244,\n",
              " '_dropouts_0.500000_layer_sizes[500]_learning_rate_0.000100_n_features_1024_n_tasks_1': 0.7608009381736234,\n",
              " '_dropouts_0.200000_layer_sizes[1000]_learning_rate_0.001000_n_features_1024_n_tasks_1': 0.7693391142465218,\n",
              " '_dropouts_0.200000_layer_sizes[1000]_learning_rate_0.000100_n_features_1024_n_tasks_1': 0.7727914339604154,\n",
              " '_dropouts_0.500000_layer_sizes[1000]_learning_rate_0.001000_n_features_1024_n_tasks_1': 0.7687129507152655,\n",
              " '_dropouts_0.500000_layer_sizes[1000]_learning_rate_0.000100_n_features_1024_n_tasks_1': 0.7483144106407997,\n",
              " '_dropouts_0.200000_layer_sizes[1000, 1000]_learning_rate_0.001000_n_features_1024_n_tasks_1': 0.7345441713207916,\n",
              " '_dropouts_0.200000_layer_sizes[1000, 1000]_learning_rate_0.000100_n_features_1024_n_tasks_1': 0.7753420169508133,\n",
              " '_dropouts_0.500000_layer_sizes[1000, 1000]_learning_rate_0.001000_n_features_1024_n_tasks_1': 0.7384488903586125,\n",
              " '_dropouts_0.500000_layer_sizes[1000, 1000]_learning_rate_0.000100_n_features_1024_n_tasks_1': 0.7580176489320007}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#all_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHBcTkaYGWoD"
      },
      "source": [
        "We can see a few general patterns.  Using two layers with the larger learning rate doesn't work very well.  It seems the deeper model requires a smaller learning rate.  We also see that 20% dropout usually works better than 50%.  Once we narrow down the list of models based on these observations, all the validation scores are very close to each other, probably close enough that the remaining variation is mainly noise.  It doesn't seem to make much difference which of the remaining hyperparameter sets we use, so let's arbitrarily pick a single layer of width 1000 and learning rate of 0.0001.\n",
        "\n",
        "## Early Stopping\n",
        "\n",
        "There is one other important hyperparameter we haven't considered yet: how long we train the model for.  `GridHyperparamOpt` trains each for a fixed, fairly small number of epochs.  That isn't necessarily the best number.\n",
        "\n",
        "You might expect that the longer you train, the better your model will get, but that isn't usually true.  If you train too long, the model will usually start overfitting to irrelevant details of the training set.  You can tell when this happens because the validation set score stops increasing and may even decrease, while the score on the training set continues to improve.\n",
        "\n",
        "Fortunately, we don't need to train lots of different models for different numbers of steps to identify the optimal number.  We just train it once, monitor the validation score, and keep whichever parameters maximize it.  This is called \"early stopping\".  DeepChem's `ValidationCallback` class can do this for us automatically.  In the example below, we have it compute the validation set's ROC AUC every 1000 training steps.  If you add the `save_dir` argument, it will also save a copy of the best model parameters to disk."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#My own code snippet which involves training once as opposed to hyperparameters optimization presented in this collab. I would keep on checking to see why it fails\n",
        "import numpy as np\n",
        "import deepchem as dc\n",
        "\n",
        "model = dc.models.MultitaskClassifier(n_tasks=len(tasks), n_features = 1024, layer_sizes = [1000])\n",
        "model.fit(train_dataset, nb_epoch = 10)\n",
        "\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
        "train_scores = model.evaluate(train_dataset, [metric], transformers)\n",
        "test_scores = model.evaluate(test_dataset, [metric], transformers)\n",
        "print(train_scores)\n",
        "print(test_scores)"
      ],
      "metadata": {
        "id": "2n1mOOYAs6OH",
        "outputId": "4e83dd39-e497-4689-a062-3b69a11f7c03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mean-roc_auc_score': 0.9876401943007891}\n",
            "{'mean-roc_auc_score': 0.7402223874543734}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fKoduMNRGWoD",
        "outputId": "12f0490b-8357-4b24-edd1-5aabf0fb97e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000 validation: mean-roc_auc_score=0.748017\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'Metric' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-727b0cb90c8d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultitaskClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValidationCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/torch_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0maverage\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mover\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmost\u001b[0m \u001b[0mrecent\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         return self.fit_generator(\n\u001b[0m\u001b[1;32m    339\u001b[0m             self.default_generator(dataset,\n\u001b[1;32m    340\u001b[0m                                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/torch_model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_checkpoints_to_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 self._log_scalar_to_tensorboard('loss', batch_loss,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepchem/models/callbacks.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model, step)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 model._log_scalar_to_tensorboard(key, scores[key],\n\u001b[1;32m     89\u001b[0m                                                  model.get_global_step())\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_metric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_on_minimum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Metric' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "#This block of code returns the TypeError: 'Metric' object is not subscriptable\n",
        "\n",
        "# model = dc.models.MultitaskClassifier(n_tasks=len(tasks), n_features=1024, layer_sizes=[1000], dropouts=0.2, learning_rate=0.0001)\n",
        "# callback = dc.models.ValidationCallback(valid_dataset, 1000, metric)\n",
        "# model.fit(train_dataset, nb_epoch=50, callbacks=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af-CqbDnGWoE"
      },
      "source": [
        "## Learning Rate Schedules\n",
        "\n",
        "In the examples above we use a fixed learning rate throughout training.  In some cases it works better to vary the learning rate during training.  To do this in DeepChem, we simply specify a `LearningRateSchedule` object instead of a number for the `learning_rate` argument.  In the following example we use a learning rate that decreases exponentially.  It starts at 0.0002, then gets multiplied by 0.9 after every 1000 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px-gMF6XGWoE"
      },
      "outputs": [],
      "source": [
        "learning_rate = dc.models.optimizers.ExponentialDecay(0.0002, 0.9, 1000)\n",
        "model = dc.models.MultitaskClassifier(n_tasks=len(tasks),\n",
        "                                      n_features=1024,\n",
        "                                      layer_sizes=[1000],\n",
        "                                      dropouts=0.2,\n",
        "                                      learning_rate=learning_rate)\n",
        "model.fit(train_dataset, nb_epoch=50, callbacks=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssi6cBmh12z"
      },
      "source": [
        "# Congratulations! Time to join the Community!\n",
        "\n",
        "Congratulations on completing this tutorial notebook! If you enjoyed working through the tutorial, and want to continue working with DeepChem, we encourage you to finish the rest of the tutorials in this series. You can also help the DeepChem community in the following ways:\n",
        "\n",
        "## Star DeepChem on [GitHub](https://github.com/deepchem/deepchem)\n",
        "This helps build awareness of the DeepChem project and the tools for open source drug discovery that we're trying to build.\n",
        "\n",
        "## Join the DeepChem Discord\n",
        "The DeepChem [Discord](https://discord.gg/cGzwCdrUqS) hosts a number of scientists, developers, and enthusiasts interested in deep learning for the life sciences. Join the conversation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOBd6-YdQSvF"
      },
      "source": [
        "## Citing This Tutorial\n",
        "If you found this tutorial useful please consider citing it using the provided BibTeX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZUk_9yIYw0c"
      },
      "outputs": [],
      "source": [
        "@manual{Intro9,\n",
        " title={Advanced Model Training},\n",
        " organization={DeepChem},\n",
        " author={Eastman, Peter and Ramsundar, Bharath},\n",
        " howpublished = {\\url{https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Advanced_Model_Training.ipynb}},\n",
        " year={2021},\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "06_Going_Deeper_on_Molecular_Featurizations.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}