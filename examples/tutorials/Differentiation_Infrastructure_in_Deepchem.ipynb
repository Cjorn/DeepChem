{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Differentiation Capabilities in Deepchem**\n",
    "\n",
    "Author : Rakshit Kr Singh : [Website](https://greatrsingh.in/) : [LinkedIn](https://www.linkedin.com/in/rakshit-singh-ai/) : [GitHub](https://github.com/GreatRSingh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for solving Non Linear Equations\n",
    "\n",
    "<h4>\n",
    "Non-linear equations are essential across various disciplines, including physics, engineering, economics, biology, and finance. They describe complex relationships and phenomena that cannot be adequately modeled with linear equations. From gravitational interactions in celestial bodies to biochemical reactions in living organisms, non-linear equations play a vital role in understanding and predicting real-world systems. Whether it's optimizing structures, analyzing market dynamics, or designing machine learning algorithms, solving non-linear equations is fundamental for advancing scientific knowledge and technological innovation.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broyden1 method\n",
    "\n",
    "<h5>\n",
    "Find a root of a function, using Broyden’s first Jacobian approximation.\n",
    "This method is also known as \"Broyden’s good method\".\n",
    "\n",
    "The Broyden method is a numerical algorithm used to solve systems of nonlinear equations.\n",
    "It's an iterative method that updates an approximation to the Jacobian matrix, which\n",
    "represents the system's derivatives with respect to the variables. Instead of recalculating\n",
    "the Jacobian matrix at each iteration, the Broyden method updates it using rank-one corrections.\n",
    "</h5>\n",
    "\n",
    "References\n",
    "\n",
    "[1].. \"A class of methods for solving nonlinear simultaneous equations\" by Charles G. Broyden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.1623],\n",
       "         [-3.1623]]),\n",
       " tensor([-9.5367e-07, -9.5367e-07]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils import rootfinder\n",
    "def func1(x, A):\n",
    "    return x[0] * x[1] - A\n",
    "y0 = torch.zeros((2, 1))\n",
    "A = torch.tensor([10, 10])\n",
    "yroot = rootfinder(func1, y0, params=(A,), method='broyden1')\n",
    "yroot, func1(yroot, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broyden2 Method\n",
    "\n",
    "<h5>\n",
    "The Broyden2 method is a specific variant within the Broyden family of methods.\n",
    "\n",
    "The Broyden2 method is an algorithm used to find the root of a function.\n",
    "It's a quasi-Newton method, which means it iteratively improves an\n",
    "approximation of the function's Jacobian (the matrix that contains all its\n",
    "first-order partial derivatives) to solve the system of equations where\n",
    "the function itself is zero.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0459],\n",
       "         [-0.0663]], grad_fn=<_RootFinderBackward>),\n",
       " tensor([[ 1.0300e-06],\n",
       "         [-3.2783e-07]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils import rootfinder\n",
    "def func1(y, A):\n",
    "    return torch.tanh(A @ y + 0.1) + y / 2.0\n",
    "A = torch.tensor([[1.1, 0.4], [0.3, 0.8]]).requires_grad_()\n",
    "y0 = torch.zeros((2,1))\n",
    "yroot = rootfinder(func1, y0, params=(A,), method='broyden2')\n",
    "yroot, func1(yroot, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizer\n",
    "\n",
    "<h5>\n",
    "Minimization algorithms, including variants of gradient descent like ADAM, are fundamental tools in various fields of science, engineering, and optimization. Here are some common usage scenarios and applications of minimization algorithms:\n",
    "<br>\n",
    "\n",
    "1. Function Optimization: Minimization algorithms are used to find the minimum (or maximum) of a function. This is applicable in various contexts such as mathematical optimization, engineering design, finance (e.g., portfolio optimization), and physics (e.g., minimizing energy functions).\n",
    "\n",
    "2. Global Optimization: Minimization algorithms are employed in global optimization problems where the goal is to find the global minimum of a function within a given domain. Evolutionary algorithms, simulated annealing, and particle swarm optimization are examples of algorithms used for global optimization.\n",
    "\n",
    "3. Nonlinear Optimization: Minimization algorithms are crucial for solving nonlinear optimization problems, where the objective function or constraints are nonlinear. These problems arise in engineering design, process optimization, and operations research.\n",
    "\n",
    "4. Quantum Computing: Minimization algorithms play a role in quantum computing, particularly in quantum optimization and quantum machine learning. Quantum annealing and variational quantum algorithms are examples of techniques used for optimization tasks.\n",
    "\n",
    "5. Control Systems: Minimization algorithms are used in control systems engineering to optimize control strategies and parameters for achieving desired system behavior and performance.\n",
    "\n",
    "These are just a few examples of the wide-ranging applications of minimization algorithms. They are essential tools for solving optimization problems in diverse fields, ranging from scientific research and engineering to finance and machine learning.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9973), (tensor(3.0000), tensor(-0.0053)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils.optimize.minimizer import gd\n",
    "def fcn(x):\n",
    "    return 2 * x + (x - 2) ** 2, 2 * (x - 2) + 2\n",
    "x0 = torch.tensor(0.0, requires_grad=True)\n",
    "x = gd(fcn, x0, [])\n",
    "x, fcn(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
