{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Differentiation Capabilities in Deepchem**\n",
    "\n",
    "Author : Rakshit Kr Singh : [Website](https://greatrsingh.in/) : [LinkedIn](https://www.linkedin.com/in/rakshit-singh-ai/) : [GitHub](https://github.com/GreatRSingh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for solving Non Linear Equations\n",
    "\n",
    "<h5>\n",
    "Nonlinear Equations (NLE):\n",
    "Nonlinear equations are mathematical expressions where the relationship between the variables is not linear. Unlike linear equations, which have a constant rate of change, nonlinear equations involve terms with higher powers or functions like exponentials, logarithms, trigonometric functions, etc.\n",
    "\n",
    "A simple NLE is y = x^2\n",
    "<br>\n",
    "<br>\n",
    "Non-linear equations are essential across various disciplines, including physics, engineering, economics, biology, and finance. They describe complex relationships and phenomena that cannot be adequately modeled with linear equations. From gravitational interactions in celestial bodies to biochemical reactions in living organisms, non-linear equations play a vital role in understanding and predicting real-world systems. Whether it's optimizing structures, analyzing market dynamics, or designing machine learning algorithms, solving non-linear equations is fundamental for advancing scientific knowledge and technological innovation.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broyden1 method\n",
    "\n",
    "<h5>\n",
    "Broyden's First Method, is an iterative numerical method used for solving systems of nonlinear equations. It's particularly useful when the Jacobian matrix (the matrix of partial derivatives of the equations) is difficult or expensive to compute. \n",
    "<br>\n",
    "<br>\n",
    "Broyden's Method is an extension of the Secant Method for systems of nonlinear equations. It iteratively updates an approximation to the Jacobian matrix using the information from previous iterations. The algorithm converges to the solution by updating the variables in the direction that minimizes the norm of the system of equations.\n",
    "<br>\n",
    "<br>\n",
    "Advantages:\n",
    "\n",
    "- Does not require explicit computation of the Jacobian matrix, which can be computationally expensive or difficult to obtain.\n",
    "- Can converge to solutions even for systems with singular or ill-conditioned Jacobians.\n",
    "- Suitable for large-scale systems of nonlinear equations.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Convergence may be slower compared to methods that utilize the true Jacobian matrix.\n",
    "- Convergence behavior may be sensitive to the choice of initial approximation to the Jacobian and the update formula.\n",
    "<br>\n",
    "<br>\n",
    "$e^{i\\pi}+1=0$\n",
    "\n",
    "$$ \\frac{1}{n}^n$$\n",
    "</h5>\n",
    "\n",
    "References\n",
    "\n",
    "[1].. \"A class of methods for solving nonlinear simultaneous equations\" by Charles G. Broyden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.1623],\n",
       "         [-3.1623]]),\n",
       " tensor([-9.5367e-07, -9.5367e-07]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils import rootfinder\n",
    "def func1(x, A):\n",
    "    return x[0] * x[1] - A\n",
    "y0 = torch.zeros((2, 1))\n",
    "A = torch.tensor([10, 10])\n",
    "yroot = rootfinder(func1, y0, params=(A,), method='broyden1')\n",
    "yroot, func1(yroot, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broyden2 Method\n",
    "\n",
    "<h5>\n",
    "The Broyden2 method is a specific variant within the Broyden family of methods.\n",
    "\n",
    "The Broyden2 method is an algorithm used to find the root of a function.\n",
    "It's a quasi-Newton method, which means it iteratively improves an\n",
    "approximation of the function's Jacobian (the matrix that contains all its\n",
    "first-order partial derivatives) to solve the system of equations where\n",
    "the function itself is zero.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0459],\n",
       "         [-0.0663]], grad_fn=<_RootFinderBackward>),\n",
       " tensor([[ 1.0300e-06],\n",
       "         [-3.2783e-07]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils import rootfinder\n",
    "def func1(y, A):\n",
    "    return torch.tanh(A @ y + 0.1) + y / 2.0\n",
    "A = torch.tensor([[1.1, 0.4], [0.3, 0.8]]).requires_grad_()\n",
    "y0 = torch.zeros((2,1))\n",
    "yroot = rootfinder(func1, y0, params=(A,), method='broyden2')\n",
    "yroot, func1(yroot, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizer\n",
    "\n",
    "<h5>\n",
    "Minimization algorithms, including variants of gradient descent like ADAM, are fundamental tools in various fields of science, engineering, and optimization. Here are some common usage scenarios and applications of minimization algorithms:\n",
    "<br>\n",
    "\n",
    "- Function Optimization: Minimization algorithms are used to find the minimum (or maximum) of a function. This is applicable in various contexts such as mathematical optimization, engineering design, finance (e.g., portfolio optimization), and physics (e.g., minimizing energy functions).\n",
    "\n",
    "- Global Optimization: Minimization algorithms are employed in global optimization problems where the goal is to find the global minimum of a function within a given domain. Evolutionary algorithms, simulated annealing, and particle swarm optimization are examples of algorithms used for global optimization.\n",
    "\n",
    "- Nonlinear Optimization: Minimization algorithms are crucial for solving nonlinear optimization problems, where the objective function or constraints are nonlinear. These problems arise in engineering design, process optimization, and operations research.\n",
    "\n",
    "- Quantum Computing: Minimization algorithms play a role in quantum computing, particularly in quantum optimization and quantum machine learning. Quantum annealing and variational quantum algorithms are examples of techniques used for optimization tasks.\n",
    "\n",
    "- Control Systems: Minimization algorithms are used in control systems engineering to optimize control strategies and parameters for achieving desired system behavior and performance.\n",
    "\n",
    "These are just a few examples of the wide-ranging applications of minimization algorithms. They are essential tools for solving optimization problems in diverse fields, ranging from scientific research and engineering to finance and machine learning.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent (GD)\n",
    "\n",
    "<h5>\n",
    "Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function.\n",
    "<br>\n",
    "<br>\n",
    "Gradient Descent is a fundamental optimization algorithm used to minimize the cost function in various machine learning and optimization problems. It iteratively updates the parameters in the direction of the negative gradient of the cost function.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9973), (tensor(3.0000), tensor(-0.0053)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils.optimize.minimizer import gd\n",
    "def fcn(x):\n",
    "    return 2 * x + (x - 2) ** 2, 2 * (x - 2) + 2\n",
    "x0 = torch.tensor(0.0, requires_grad=True)\n",
    "x = gd(fcn, x0, [])\n",
    "x, fcn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.1687), (tensor(-34.1530), tensor(22.0812)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils.optimize.minimizer import adam\n",
    "def fcn(x):\n",
    "    return 2 * x + (x - 2) ** 3, 2 * (x - 2)**2 + 2\n",
    "x0 = torch.tensor(0.0, requires_grad=True)\n",
    "x = adam(fcn, x0, [])\n",
    "x, fcn(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
