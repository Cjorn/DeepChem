{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForUniversalSegmentation, OneFormerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OneFormerConfig(encoder_layers=2, decoder_layers=2, num_attention_heads=2)\n",
    "model = AutoModelForUniversalSegmentation.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneFormerForUniversalSegmentation(\n",
       "  (model): OneFormerModel(\n",
       "    (pixel_level_module): OneFormerPixelLevelModule(\n",
       "      (encoder): SwinBackbone(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.3)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.3)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-5): 6 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.3)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.3)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (hidden_states_norms): ModuleDict(\n",
       "          (stage1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder): OneFormerPixelDecoder(\n",
       "        (position_embedding): OneFormerSinePositionEmbedding()\n",
       "        (input_projections): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder): OneFormerPixelDecoderEncoderOnly(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x OneFormerPixelDecoderEncoderLayer(\n",
       "              (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
       "                (sampling_offsets): Linear(in_features=256, out_features=48, bias=True)\n",
       "                (attention_weights): Linear(in_features=256, out_features=24, bias=True)\n",
       "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (adapter_1): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer_module): OneFormerTransformerModule(\n",
       "      (position_embedder): OneFormerSinePositionEmbedding()\n",
       "      (queries_embedder): Embedding(150, 256)\n",
       "      (decoder): OneFormerTransformerDecoder(\n",
       "        (query_transformer): OneFormerTransformerDecoderQueryTransformer(\n",
       "          (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (multihead_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "                (dropout3): Dropout(p=0.1, inplace=False)\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): OneFormerTransformerDecoderLayer(\n",
       "            (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(\n",
       "              (self_attn): OneFormerAttention(\n",
       "                (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (ffn): OneFormerTransformerDecoderFFNLayer(\n",
       "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (query_input_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (class_embed): Linear(in_features=256, out_features=3, bias=True)\n",
       "        (mask_embed): OneFormerMLPPredictionHead(\n",
       "          (layers): Sequential(\n",
       "            (0): PredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (1): PredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (2): PredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (level_embed): Embedding(3, 256)\n",
       "    )\n",
       "    (task_encoder): OneFormerTaskModel(\n",
       "      (task_mlp): OneFormerMLPPredictionHead(\n",
       "        (layers): Sequential(\n",
       "          (0): PredictionBlock(\n",
       "            (0): Linear(in_features=77, out_features=256, bias=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (1): PredictionBlock(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (matcher): OneFormerHungarianMatcher()\n",
       "  (criterion): OneFormerLoss(\n",
       "    (matcher): OneFormerHungarianMatcher()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from deepchem.data import ImageDataset\n",
    "try:\n",
    "    import torch\n",
    "    from deepchem.models.torch_models.oneformer import OneFormer\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "@pytest.mark.torch\n",
    "def test_oneformer_train():\n",
    "    from deepchem.models.torch_models.hf_models import HuggingFaceModel\n",
    "    from transformers import OneFormerConfig\n",
    "\n",
    "    # micro config for testing\n",
    "    config = OneFormerConfig().from_pretrained(\n",
    "        'shi-labs/oneformer_ade20k_swin_tiny', is_training=True)\n",
    "    config.encoder_layers = 2\n",
    "    config.decoder_layers = 2\n",
    "    config.num_attention_heads = 2\n",
    "    config.dim_feedforward = 128\n",
    "\n",
    "    model = OneFormer(model_path='shi-labs/oneformer_ade20k_swin_tiny',\n",
    "                      model_config=config,\n",
    "                      segmentation_task=\"semantic\",\n",
    "                      torch_dtype=torch.float16,\n",
    "                      batch_size=1)\n",
    "    X = np.random.randint(0, 255, (3, 224, 224, 3))\n",
    "    y = np.random.randint(0, 1, (3, 224, 224))\n",
    "\n",
    "    dataset = ImageDataset(X, y)\n",
    "    avg_loss = model.fit(dataset, nb_epoch=2)\n",
    "\n",
    "    assert isinstance(model, HuggingFaceModel)\n",
    "    assert isinstance(avg_loss, float)\n",
    "\n",
    "\n",
    "@pytest.mark.torch\n",
    "def test_oneformer_predict():\n",
    "    from deepchem.models.torch_models.hf_models import HuggingFaceModel\n",
    "    from transformers import OneFormerConfig\n",
    "\n",
    "    # micro config for testing\n",
    "    config = OneFormerConfig().from_pretrained(\n",
    "        'shi-labs/oneformer_ade20k_swin_tiny', is_training=True)\n",
    "    config.encoder_layers = 2\n",
    "    config.decoder_layers = 2\n",
    "    config.num_attention_heads = 2\n",
    "    config.dim_feedforward = 128\n",
    "\n",
    "    model = OneFormer(model_path='shi-labs/oneformer_ade20k_swin_tiny',\n",
    "                      model_config=config,\n",
    "                      segmentation_task=\"semantic\",\n",
    "                      torch_dtype=torch.float16,\n",
    "                      batch_size=1)\n",
    "    X = np.random.randint(0, 255, (3, 224, 224, 3))\n",
    "    y = np.random.randint(0, 1, (3, 224, 224))\n",
    "\n",
    "    dataset = ImageDataset(X, y)\n",
    "    preds = model.predict(dataset)\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    assert isinstance(model, HuggingFaceModel)\n",
    "    assert np.array(preds).shape == y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import logging\n",
    "from collections.abc import Sequence as SequenceCollection\n",
    "from typing import Union, Dict, Tuple, Iterable, List, Optional, Callable, Any\n",
    "\n",
    "from deepchem.models.optimizers import LearningRateSchedule\n",
    "from deepchem.trans import Transformer, undo_transforms\n",
    "from deepchem.utils.typing import LossFn, OneOrMany\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from deepchem.models.torch_models import HuggingFaceModel\n",
    "    from transformers import AutoProcessor, AutoModelForUniversalSegmentation, OneFormerConfig\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"This module requires the `transformers` and the `torch` packages.\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class OneFormer(HuggingFaceModel):\n",
    "    \"\"\"\n",
    "    Wrapper class that wraps the OneFormer model as a DeepChem model.\n",
    "\n",
    "    The OneFormer model was proposed in OneFormer: One Transformer to Rule Universal Image Segmentation [1]\n",
    "    by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi. OneFormer is a universal\n",
    "    image segmentation framework that can be trained on a single panoptic dataset to perform semantic, instance,\n",
    "    and panoptic segmentation tasks.\n",
    "\n",
    "    Whilst all official HuggingFace model weights by shi-labs [2] are supported, the current implementation only\n",
    "    supports the OneFormer model for semantic segmentation.\n",
    "\n",
    "    Instance and panoptic segmentation tasks are not supported yet.\n",
    "\n",
    "    Usage Example:\n",
    "    --------------\n",
    "    >> from PIL import Image\n",
    "    >> import numpy as np\n",
    "    >> import torch\n",
    "    >> from deepchem.data import NumpyDataset, ImageDataset\n",
    "    >> from deepchem.models.torch_models import OneFormer\n",
    "    >> from deepchem.metrics import Metric, mean_absolute_error, jaccaard_index\n",
    "\n",
    "    >> # Prepare the dataset\n",
    "    >> X = np.random.randint(0, 255, (5, 512, 512, 3))\n",
    "    >> y = np.random.randint(0, 1, (5, 512, 512))\n",
    "    >> dataset = ImageDataset(X, y)\n",
    "    >> id2label = {0: \"label-A\", 1: \"label-B\"}\n",
    "\n",
    "    >> # Create the model\n",
    "    >> model = OneFormer(segmentation_task=\"semantic\", model_path='shi-labs/oneformer_ade20k_swin_tiny',\n",
    "    ...                  id2label=id2label, torch_dtype=torch.float16, batch_size=2)\n",
    "\n",
    "    >> # Train the model\n",
    "    >> avg_loss = model.fit(dataset, nb_epoch=5)\n",
    "\n",
    "    >> # Predict the model\n",
    "    >> preds = model.predict(dataset)\n",
    "\n",
    "    >> # Evaluate the model\n",
    "    >> mae_metric = Metric(mean_absolute_error)\n",
    "    >> iou_metric = Metric(jaccaard_index)\n",
    "    >> iou = iou_metric.compute_metric(dataset.y, preds)\n",
    "    >> mae = mae_metric.compute_metric(np.array(dataset.y).flatten(), np.array(preds).flatten())\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Jain, J., Li, J., Chiu, M., Hassani, A., Orlov, N., & Shi, H. (2022, November 10). OneFormer: One Transformer to rule universal image segmentation. arXiv.org. https://arxiv.org/abs/2211.06220\n",
    "    .. [2] https://huggingface.co/shi-labs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 segmentation_task: str = \"semantic\",\n",
    "                 model_path: str = 'shi-labs/oneformer_ade20k_swin_tiny',\n",
    "                 model_config: Optional[OneFormerConfig] = None,\n",
    "                 model_processor: Optional[AutoProcessor] = None,\n",
    "                 id2label: Dict = {\n",
    "                     0: \"unlabelled\",\n",
    "                     1: \"labelled\"\n",
    "                 },\n",
    "                 torch_dtype: torch.dtype = torch.float32,\n",
    "                 **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        segmentation_task: str\n",
    "            The segmentation task to perform. The available tasks are\n",
    "            - `semantic` - semantic segmentation (default)\n",
    "            - `instance` - instance segmentation (not supported yet)\n",
    "            - `panoptic` - panoptic segmentation (not supported yet)\n",
    "        model_path: str\n",
    "            Path to the OneFormer HuggingFace model; HF Model Hub or local\n",
    "            - ex: 'shi-labs/oneformer_ade20k_swin_tiny'\n",
    "        model_config: OneFormerConfig\n",
    "            Optional configuration for the OneFormer model. If not provided, the configuration\n",
    "            will be loaded from the model_path. If provided, the configuration will be used\n",
    "            to initialize the model instead of the configuration from the model_path.\n",
    "        model_processor: AutoProcessor\n",
    "            Optional processor for the OneFormer model. If not provided, the processor\n",
    "            will be loaded from the model_path. If provided, the processor will be used\n",
    "            to initialize the model instead of the processor from the model_path.\n",
    "        id2label: dict\n",
    "            A dictionary mapping class indices to class labels.\n",
    "        torch_dtype: torch.dtype\n",
    "            The torch data type to use for the model. The supported data types are\n",
    "            - `torch.float32` (default)\n",
    "            - `torch.float16`\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        If a custom model configuration and processor are provided, ensure that\n",
    "        `model_processor.image_processor.num_text == model_config.num_queries - model_config.text_encoder_n_ctx` for\n",
    "        contrastive learning to work correctly during training.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.segmentation_task = segmentation_task\n",
    "        self.task = \"universal_segmentation\"\n",
    "        self.id2label = id2label\n",
    "        self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "        self.num_labels = len(self.id2label)\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "        if model_config is None and model_path is not None:\n",
    "            self.model_config = OneFormerConfig().from_pretrained(\n",
    "                model_path, is_training=True, torch_dtype=torch_dtype)\n",
    "        else:\n",
    "            self.model_config = model_config\n",
    "\n",
    "        if model_processor is None and model_path is not None:\n",
    "            self.model_processor = AutoProcessor.from_pretrained(model_path)\n",
    "        else:\n",
    "            self.model_processor = model_processor\n",
    "\n",
    "        if (self.model_path is None) and (self.model_config is None or\n",
    "                                          self.model_processor is None):\n",
    "            raise ValueError(\n",
    "                \"Please provide either a model path or a model configuration and processor.\"\n",
    "            )\n",
    "\n",
    "        self.model_config.id2label = self.id2label\n",
    "        self.model_config.label2id = self.label2id\n",
    "\n",
    "        self.model = AutoModelForUniversalSegmentation.from_config(\n",
    "            self.model_config)\n",
    "\n",
    "        self.model_processor.image_processor.num_text = self.model.config.num_queries - self.model.config.text_encoder_n_ctx\n",
    "        assert self.model_processor.image_processor.num_text == self.model.config.num_queries - self.model.config.text_encoder_n_ctx\n",
    "\n",
    "        super().__init__(model=self.model,\n",
    "                         task=self.task,\n",
    "                         tokenizer=None,\n",
    "                         **kwargs)\n",
    "\n",
    "    def _prepare_batch(self, batch: Tuple[Any, Any, Any]):\n",
    "        \"\"\"\n",
    "        Preprocess and prepare the batch for the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple\n",
    "            A tuple of the form (X, y, w) where X is the input data, y is the label and w is the weight.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_inputs: Dict\n",
    "            A dictionary containing the processed inputs.\n",
    "        y: torch.Tensor\n",
    "            The label tensor.\n",
    "        w: torch.Tensor\n",
    "            The weight tensor.\n",
    "        \"\"\"\n",
    "        X, y, w = batch\n",
    "        if y is not None:\n",
    "            X, y, w = X[0], y[0], w[0]\n",
    "        else:\n",
    "            X = X[0]\n",
    "\n",
    "        images = []\n",
    "        for x in X:\n",
    "            images.append(Image.fromarray(x.astype(np.uint8)))\n",
    "\n",
    "        self.image_size = images[0].size\n",
    "\n",
    "        if y is not None:\n",
    "            masks = y.astype(np.uint8)\n",
    "        else:\n",
    "            masks = None\n",
    "\n",
    "        processed_inputs: Dict[str, torch.Tensor]\n",
    "        processed_inputs = {}\n",
    "        for idx, img in enumerate(images):\n",
    "            if masks is not None:\n",
    "                inputs = self.model_processor(\n",
    "                    images=images[idx],\n",
    "                    segmentation_maps=masks[idx],\n",
    "                    task_inputs=[self.segmentation_task],\n",
    "                    return_tensors=\"pt\")\n",
    "            else:\n",
    "                inputs = self.model_processor(\n",
    "                    images=images[idx],\n",
    "                    task_inputs=[self.segmentation_task],\n",
    "                    return_tensors=\"pt\")\n",
    "            # Process and append to the same dictionary\n",
    "            for k, v in inputs.items():\n",
    "                v = v.squeeze().to(self.device) if isinstance(\n",
    "                    v, torch.Tensor) else v[0]\n",
    "\n",
    "                # If this is the first time we're adding to processed_inputs, initialize it with the correct batch shape\n",
    "                if k in processed_inputs:\n",
    "                    processed_inputs[k] = torch.cat(\n",
    "                        (processed_inputs[k], v.unsqueeze(0)), dim=0)\n",
    "                else:\n",
    "                    processed_inputs[k] = v.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        return processed_inputs, y, w\n",
    "\n",
    "    def fit_generator(self,\n",
    "                      generator: Iterable[Tuple[Any, Any, Any]],\n",
    "                      max_checkpoints_to_keep: int = 5,\n",
    "                      checkpoint_interval: int = 1000,\n",
    "                      restore: bool = False,\n",
    "                      variables: Optional[Union[List[torch.nn.Parameter],\n",
    "                                                torch.nn.ParameterList]] = None,\n",
    "                      loss: Optional[LossFn] = None,\n",
    "                      callbacks: Union[Callable, List[Callable]] = [],\n",
    "                      all_losses: Optional[List[float]] = None) -> float:\n",
    "        \"\"\"Train this model on data from a generator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generator: generator\n",
    "            this should generate batches, each represented as a tuple of the form\n",
    "            (inputs, labels, weights).\n",
    "        max_checkpoints_to_keep: int\n",
    "            the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
    "        checkpoint_interval: int\n",
    "            the frequency at which to write checkpoints, measured in training steps.\n",
    "            Set this to 0 to disable automatic checkpointing.\n",
    "        restore: bool\n",
    "            if True, restore the model from the most recent checkpoint and continue training\n",
    "            from there.  If False, retrain the model from scratch.\n",
    "        variables: list of torch.nn.Parameter\n",
    "            the variables to train.  If None (the default), all trainable variables in\n",
    "            the model are used.\n",
    "        loss: function\n",
    "            a function of the form f(outputs, labels, weights) that computes the loss\n",
    "            for each batch.  If None (the default), the model's standard loss function\n",
    "            is used.\n",
    "        callbacks: function or list of functions\n",
    "            one or more functions of the form f(model, step, **kwargs) that will be invoked\n",
    "            after every step.  This can be used to perform validation, logging, etc.\n",
    "        all_losses: Optional[List[float]], optional (default None)\n",
    "            If specified, all logged losses are appended into this list. Note that\n",
    "            you can call `fit()` repeatedly with the same list and losses will\n",
    "            continue to be appended.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The average loss over the most recent checkpoint interval\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        A HuggingFace model can return embeddings (last hidden state), attentions.\n",
    "        Support must be added to return the embeddings to the user, so that it can\n",
    "        be used for other downstream applications.\n",
    "        \"\"\"\n",
    "        if not isinstance(callbacks, SequenceCollection):\n",
    "            callbacks = [callbacks]\n",
    "        self._ensure_built()\n",
    "        self.model.train()\n",
    "        self.model.model.is_training = True\n",
    "        avg_loss = 0.0\n",
    "        last_avg_loss = 0.0\n",
    "        averaged_batches = 0\n",
    "        if variables is None:\n",
    "            optimizer = self._pytorch_optimizer\n",
    "            lr_schedule = self._lr_schedule\n",
    "        else:\n",
    "            var_key = tuple(variables)\n",
    "            if var_key in self._optimizer_for_vars:\n",
    "                optimizer, lr_schedule = self._optimizer_for_vars[var_key]\n",
    "            else:\n",
    "                optimizer = self.optimizer._create_pytorch_optimizer(variables)\n",
    "                if isinstance(self.optimizer.learning_rate,\n",
    "                              LearningRateSchedule):\n",
    "                    lr_schedule = self.optimizer.learning_rate._create_pytorch_schedule(\n",
    "                        optimizer)\n",
    "                else:\n",
    "                    lr_schedule = None\n",
    "                self._optimizer_for_vars[var_key] = (optimizer, lr_schedule)\n",
    "        time1 = time.time()\n",
    "\n",
    "        # Main training loop.\n",
    "        for batch in generator:\n",
    "            if restore:\n",
    "                self.restore()\n",
    "                restore = False\n",
    "\n",
    "            inputs, labels, weights = self._prepare_batch(batch)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "            batch_loss = outputs.get(\"loss\")\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_schedule is not None:\n",
    "                lr_schedule.step()\n",
    "            self._global_step += 1\n",
    "            current_step = self._global_step\n",
    "\n",
    "            avg_loss += batch_loss\n",
    "\n",
    "            # Report progress and write checkpoints.\n",
    "            averaged_batches += 1\n",
    "            should_log = (current_step % self.log_frequency == 0)\n",
    "            if should_log:\n",
    "                avg_loss = float(avg_loss) / averaged_batches\n",
    "                logger.info('Ending global_step %d: Average loss %g' %\n",
    "                            (current_step, avg_loss))\n",
    "                if all_losses is not None:\n",
    "                    all_losses.append(avg_loss)\n",
    "                # Capture the last avg_loss in case of return since we're resetting to 0 now\n",
    "                last_avg_loss = avg_loss\n",
    "                avg_loss = 0.0\n",
    "                averaged_batches = 0\n",
    "\n",
    "            if checkpoint_interval > 0 and current_step % checkpoint_interval == checkpoint_interval - 1:\n",
    "                self.save_checkpoint(max_checkpoints_to_keep)\n",
    "            for c in callbacks:\n",
    "                try:\n",
    "                    # NOTE In DeepChem > 2.8.0, callback signature is updated to allow\n",
    "                    # variable arguments.\n",
    "                    c(self, current_step, iteration_loss=batch_loss)\n",
    "                except TypeError:\n",
    "                    # DeepChem <= 2.8.0, the callback should have this signature.\n",
    "                    c(self, current_step)\n",
    "            if self.tensorboard and should_log:\n",
    "                self._log_scalar_to_tensorboard('loss', batch_loss,\n",
    "                                                current_step)\n",
    "            if (self.wandb_logger is not None) and should_log:\n",
    "                all_data = dict({'train/loss': batch_loss})\n",
    "                self.wandb_logger.log_data(all_data, step=current_step)\n",
    "\n",
    "        # Report final results.\n",
    "        if averaged_batches > 0:\n",
    "            avg_loss = float(avg_loss) / averaged_batches\n",
    "            logger.info('Ending global_step %d: Average loss %g' %\n",
    "                        (current_step, avg_loss))\n",
    "            if all_losses is not None:\n",
    "                all_losses.append(avg_loss)\n",
    "            last_avg_loss = avg_loss\n",
    "\n",
    "        if checkpoint_interval > 0:\n",
    "            self.save_checkpoint(max_checkpoints_to_keep)\n",
    "\n",
    "        time2 = time.time()\n",
    "        logger.info(\"TIMING: model fitting took %0.3f s\" % (time2 - time1))\n",
    "        return last_avg_loss\n",
    "\n",
    "    def _predict(self, generator: Iterable[Tuple[Any, Any, Any]],\n",
    "                 transformers: List[Transformer], uncertainty: bool,\n",
    "                 other_output_types: Optional[OneOrMany[str]]):\n",
    "        \"\"\"Predicts output for data provided by generator.\n",
    "\n",
    "        This is the private implementation of prediction. Do not\n",
    "        call it directly. Instead call one of the public prediction methods.\n",
    "        Please use model.predict() instead.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generator: generator\n",
    "            this should generate batches, each represented as a tuple of the form\n",
    "            (inputs, labels, weights).\n",
    "        transformers: list of dc.trans.Transformers\n",
    "            Transformers that the input data has been transformed by.  The output\n",
    "            is passed through these transformers to undo the transformations.\n",
    "        uncertainty: bool\n",
    "            specifies whether this is being called as part of estimating uncertainty.\n",
    "            If True, it sets the training flag so that dropout will be enabled, and\n",
    "            returns the values of the uncertainty outputs.\n",
    "        other_output_types: list, optional\n",
    "            Provides a list of other output_types (strings) to predict from model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            A NumPy array of the model produces a single output, or a list of arrays\n",
    "            if it produces multiple outputs\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        A HuggingFace model does not output uncertainity. The argument is here\n",
    "        since it is also present in TorchModel. Similarly, other variables like\n",
    "        other_output_types are also not used. Instead, a HuggingFace model outputs\n",
    "        loss, logits, hidden state and attentions.\n",
    "        \"\"\"\n",
    "        results: Optional[List[List[np.ndarray]]] = None\n",
    "        variances: Optional[List[List[np.ndarray]]] = None\n",
    "        if uncertainty and (other_output_types is not None):\n",
    "            raise ValueError(\n",
    "                'This model cannot compute uncertainties and other output types simultaneously. Please invoke one at a time.'\n",
    "            )\n",
    "        if uncertainty:\n",
    "            if self._variance_outputs is None or len(\n",
    "                    self._variance_outputs) == 0:\n",
    "                raise ValueError('This model cannot compute uncertainties')\n",
    "            if len(self._variance_outputs) != len(self._prediction_outputs):\n",
    "                raise ValueError(\n",
    "                    'The number of variances must exactly match the number of outputs'\n",
    "                )\n",
    "        if other_output_types:\n",
    "            if self._other_outputs is None or len(self._other_outputs) == 0:\n",
    "                raise ValueError(\n",
    "                    'This model cannot compute other outputs since no other output_types were specified.'\n",
    "                )\n",
    "        self._ensure_built()\n",
    "        self.model.eval()\n",
    "        self.model.model.is_training = False\n",
    "\n",
    "        for batch in generator:\n",
    "            inputs, labels, weights = batch\n",
    "            inputs, _, _ = self._prepare_batch((inputs, None, None))\n",
    "\n",
    "            output_values = []\n",
    "            # Invoke the model.\n",
    "            with torch.no_grad():\n",
    "                output = self.model(**inputs)\n",
    "                output_values.append(output)\n",
    "\n",
    "            # Post process the output\n",
    "            for idx, i in enumerate(output_values):\n",
    "                if self.segmentation_task == \"semantic\":\n",
    "                    output_values[\n",
    "                        idx] = self.model_processor.post_process_semantic_segmentation(\n",
    "                            output_values[idx],\n",
    "                            target_sizes=[self.image_size[::-1]])[0]\n",
    "\n",
    "            if isinstance(output_values, torch.Tensor):\n",
    "                output_values = [output_values]\n",
    "            output_values = [t.detach().cpu().numpy() for t in output_values]\n",
    "\n",
    "            # Apply tranformers and record results.\n",
    "            if uncertainty:\n",
    "                var = [output_values[i] for i in self._variance_outputs]\n",
    "                if variances is None:\n",
    "                    variances = [var]\n",
    "                else:\n",
    "                    for i, t in enumerate(var):\n",
    "                        variances[i].append(t)\n",
    "            access_values = []\n",
    "            if other_output_types:\n",
    "                access_values += self._other_outputs\n",
    "            elif self._prediction_outputs is not None:\n",
    "                access_values += self._prediction_outputs\n",
    "\n",
    "            if len(access_values) > 0:\n",
    "                output_values = [output_values[i] for i in access_values]\n",
    "\n",
    "            if len(transformers) > 0:\n",
    "                if len(output_values) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"predict() does not support Transformers for models with multiple outputs.\"\n",
    "                    )\n",
    "                elif len(output_values) == 1:\n",
    "                    output_values = [\n",
    "                        undo_transforms(output_values[0], transformers)\n",
    "                    ]\n",
    "            if results is None:\n",
    "                results = [[] for i in range(len(output_values))]\n",
    "            for i, t in enumerate(output_values):\n",
    "                results[i].append(t)\n",
    "\n",
    "        # Concatenate arrays to create the final results\n",
    "        if results is not None:\n",
    "            final_results = results\n",
    "        if variances is not None:\n",
    "            final_variances = variances\n",
    "\n",
    "        if uncertainty and variances is not None:\n",
    "            return zip(final_results, final_variances)\n",
    "\n",
    "        if len(final_results) == 1:\n",
    "            return final_results[0]\n",
    "        else:\n",
    "            return np.array(final_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
